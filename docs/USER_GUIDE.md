# DVPæ¶‚å±‚å…‰è°±å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿ - ä¿å§†çº§ä½¿ç”¨è¯´æ˜

## ç›®å½•
1. [ç³»ç»Ÿæ¦‚è¿°](#ç³»ç»Ÿæ¦‚è¿°)
2. [ç¯å¢ƒé…ç½®å’Œéƒ¨ç½²](#ç¯å¢ƒé…ç½®å’Œéƒ¨ç½²)
3. [æ¨¡å‹è®­ç»ƒå®Œæ•´æŒ‡å—](#æ¨¡å‹è®­ç»ƒå®Œæ•´æŒ‡å—)
4. [æ¨¡å‹è¯„ä¼°è¯¦ç»†æŒ‡å—](#æ¨¡å‹è¯„ä¼°è¯¦ç»†æŒ‡å—)
5. [æ¨¡å‹è°ƒä¼˜æœ€ä½³å®è·µ](#æ¨¡å‹è°ƒä¼˜æœ€ä½³å®è·µ)
6. [APIä½¿ç”¨å®Œæ•´æŒ‡å—](#apiä½¿ç”¨å®Œæ•´æŒ‡å—)
7. [æ•…éšœæ’é™¤å’ŒFAQ](#æ•…éšœæ’é™¤å’Œfaq)
8. [æ€§èƒ½ä¼˜åŒ–å»ºè®®](#æ€§èƒ½ä¼˜åŒ–å»ºè®®)

---

## ç³»ç»Ÿæ¦‚è¿°

### é¡¹ç›®ç®€ä»‹
DVPæ¶‚å±‚å…‰è°±å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿæ˜¯ä¸€ä¸ªåŸºäºæœºå™¨å­¦ä¹ çš„æ™ºèƒ½è´¨é‡æ§åˆ¶ç³»ç»Ÿï¼Œé€šè¿‡åˆ†æå…‰è°±æ•°æ®æ¥æ£€æµ‹DVPæ¶‚å±‚äº§å“çš„è´¨é‡å’Œç¨³å®šæ€§ã€‚

### æ ¸å¿ƒåŠŸèƒ½
- **Quality Scoreè®¡ç®—**: åŸºäºä¸“å®¶è§„åˆ™çš„è´¨é‡è¯„åˆ†ç³»ç»Ÿ
- **Stability Scoreè®¡ç®—**: åŸºäºè‡ªç¼–ç å™¨çš„ç¨³å®šæ€§è¯„åˆ†ç³»ç»Ÿ
- **æ™ºèƒ½å†³ç­–å¼•æ“**: å››æ ¼å†³ç­–è¡¨é€»è¾‘ï¼Œè‡ªåŠ¨åˆ†ç±»äº§å“çŠ¶æ€
- **å®æ—¶APIæœåŠ¡**: æä¾›RESTful APIæ¥å£ï¼Œæ”¯æŒå•æ¬¡å’Œæ‰¹é‡åˆ†æ
- **å¯è§†åŒ–åˆ†æ**: ä¸°å¯Œçš„å›¾è¡¨å’ŒæŠ¥å‘Šç”ŸæˆåŠŸèƒ½

### æŠ€æœ¯æ¶æ„
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   æ•°æ®è¾“å…¥å±‚     â”‚    â”‚   æ ¸å¿ƒç®—æ³•å±‚     â”‚    â”‚   åº”ç”¨æœåŠ¡å±‚     â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ å…‰è°±æ•°æ®      â”‚â”€â”€â”€â–¶â”‚ â€¢ Quality Score â”‚â”€â”€â”€â–¶â”‚ â€¢ å†³ç­–å¼•æ“      â”‚
â”‚ â€¢ æ³¢é•¿æ•°æ®      â”‚    â”‚ â€¢ Stability Scoreâ”‚    â”‚ â€¢ APIæœåŠ¡       â”‚
â”‚ â€¢ æ¶‚å±‚ç±»å‹      â”‚    â”‚ â€¢ è‡ªç¼–ç å™¨æ¨¡å‹   â”‚    â”‚ â€¢ å¯è§†åŒ–ç³»ç»Ÿ    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ç¯å¢ƒé…ç½®å’Œéƒ¨ç½²

### 1. ç³»ç»Ÿè¦æ±‚

#### æœ€ä½é…ç½®
- **æ“ä½œç³»ç»Ÿ**: Windows 10/11, macOS 10.14+, Ubuntu 18.04+
- **Pythonç‰ˆæœ¬**: 3.8+
- **å†…å­˜**: 8GB RAM
- **å­˜å‚¨**: 2GB å¯ç”¨ç©ºé—´
- **ç½‘ç»œ**: äº’è”ç½‘è¿æ¥ï¼ˆç”¨äºå®‰è£…ä¾èµ–ï¼‰

#### æ¨èé…ç½®
- **æ“ä½œç³»ç»Ÿ**: Windows 11, macOS 12+, Ubuntu 20.04+
- **Pythonç‰ˆæœ¬**: 3.9+
- **å†…å­˜**: 16GB RAM
- **å­˜å‚¨**: 5GB å¯ç”¨ç©ºé—´
- **ç½‘ç»œ**: ç¨³å®šçš„äº’è”ç½‘è¿æ¥

### 2. Pythonç¯å¢ƒå®‰è£…

#### æ­¥éª¤1: å®‰è£…Python
```bash
# Windows (æ¨èä½¿ç”¨Python 3.9æˆ–3.10)
# ä¸‹è½½åœ°å€: https://www.python.org/downloads/
# å®‰è£…æ—¶å‹¾é€‰ "Add Python to PATH"

# macOS
brew install python@3.9

# Ubuntu/Debian
sudo apt update
sudo apt install python3.9 python3.9-venv python3.9-dev
```

#### æ­¥éª¤2: éªŒè¯å®‰è£…
```bash
python --version
pip --version
```

### 3. é¡¹ç›®éƒ¨ç½²

#### æ–¹æ³•ä¸€: å¿«é€Ÿéƒ¨ç½²ï¼ˆæ¨èï¼‰
```bash
# 1. å…‹éš†æˆ–ä¸‹è½½é¡¹ç›®
# å‡è®¾é¡¹ç›®å·²è§£å‹åˆ°: /path/to/spectrum_anomaly_detection

# 2. è¿›å…¥é¡¹ç›®ç›®å½•
cd spectrum_anomaly_detection

# 3. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv

# 4. æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
# Windows:
venv\Scripts\activate
# macOS/Linux:
source venv/bin/activate

# 5. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 6. éªŒè¯å®‰è£…
python test_data_loading.py
```

#### æ–¹æ³•äºŒ: æ‰‹åŠ¨å®‰è£…ä¾èµ–
```bash
# å®‰è£…æ ¸å¿ƒä¾èµ–
pip install numpy pandas scikit-learn matplotlib seaborn
pip install joblib fastapi uvicorn pydantic
pip install openpyxl xlsxwriter

# å®‰è£…å¼€å‘ä¾èµ–ï¼ˆå¯é€‰ï¼‰
pip install jupyter pytest black flake8
```

### 4. é¡¹ç›®ç»“æ„éªŒè¯

éƒ¨ç½²å®Œæˆåï¼ŒéªŒè¯ä»¥ä¸‹ç›®å½•ç»“æ„ï¼š
```
spectrum_anomaly_detection/
â”œâ”€â”€ algorithms/          # æ ¸å¿ƒç®—æ³•
â”‚   â”œâ”€â”€ similarity_evaluator.py
â”‚   â”œâ”€â”€ decision_engine.py
â”‚   â””â”€â”€ model_cache.py
â”œâ”€â”€ data/               # æ•°æ®æ–‡ä»¶
â”‚   â”œâ”€â”€ HunterLab DVP.csv
â”‚   â””â”€â”€ data_loader.py
â”œâ”€â”€ models/             # æ¨¡å‹æ–‡ä»¶
â”‚   â””â”€â”€ DVP/
â”œâ”€â”€ scripts/            # æ‰§è¡Œè„šæœ¬
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â””â”€â”€ phase1_data_preprocessing.py
â”œâ”€â”€ api_server.py       # APIæœåŠ¡å™¨
â”œâ”€â”€ test_api.py         # APIæµ‹è¯•
â””â”€â”€ requirements.txt    # ä¾èµ–åˆ—è¡¨
```

### 5. ç¯å¢ƒæµ‹è¯•

#### åŸºç¡€åŠŸèƒ½æµ‹è¯•
```bash
# æµ‹è¯•æ•°æ®åŠ è½½
python test_data_loading.py

# æµ‹è¯•æ ¸å¿ƒç®—æ³•
python test_phase2.py

# æµ‹è¯•æ¨¡å‹è®­ç»ƒ
python test_phase3.py
```

#### APIæœåŠ¡æµ‹è¯•
```bash
# å¯åŠ¨APIæœåŠ¡å™¨
python api_server.py

# åœ¨å¦ä¸€ä¸ªç»ˆç«¯è¿è¡Œæµ‹è¯•
python test_api.py
```

---

## æ¨¡å‹è®­ç»ƒå®Œæ•´æŒ‡å—

### 1. æ•°æ®å‡†å¤‡

#### 1.1 æ•°æ®æ ¼å¼è¦æ±‚
ç³»ç»Ÿæ”¯æŒä¸¤ç§æ•°æ®æ ¼å¼ï¼š

**CSVæ ¼å¼ (æ¨è)**:
```csv
Wavelength,Sample1,Sample2,Sample3
380,0.123,0.145,0.134
381,0.124,0.146,0.135
...
780,0.234,0.256,0.245
```

**Excelæ ¼å¼**:
- ç¬¬ä¸€è¡Œ: æ³¢é•¿æ ‡é¢˜
- ç¬¬ä¸€åˆ—: æ³¢é•¿æ•°æ® (380-780nm)
- å…¶ä»–åˆ—: ä¸åŒæ ·æœ¬çš„å…‰è°±æ•°æ®

#### 1.2 æ•°æ®è´¨é‡è¦æ±‚
- **æ³¢é•¿èŒƒå›´**: 380-780nm
- **æ•°æ®ç‚¹æ•°**: å»ºè®®81ä¸ªç‚¹ï¼ˆ5nmé—´éš”ï¼‰
- **æ•°å€¼èŒƒå›´**: 0-1ä¹‹é—´çš„æµ®ç‚¹æ•°
- **ç¼ºå¤±å€¼**: ä¸å…è®¸å­˜åœ¨
- **å¼‚å¸¸å€¼**: å»ºè®®é¢„å¤„ç†å»é™¤

#### 1.3 æ•°æ®é¢„å¤„ç†ç¤ºä¾‹
```python
# å¯¼å…¥æ•°æ®é¢„å¤„ç†æ¨¡å—
from scripts.phase1_data_preprocessing import DataPreprocessor

# åˆ›å»ºé¢„å¤„ç†å™¨
preprocessor = DataPreprocessor()

# åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
data = preprocessor.load_and_preprocess('path/to/your/data.csv')

# éªŒè¯æ•°æ®è´¨é‡
quality_report = preprocessor.validate_data_quality(data)
print(quality_report)

# ä¿å­˜é¢„å¤„ç†åçš„æ•°æ®
preprocessor.save_processed_data(data, 'output/processed_data.npz')
```

### 2. è®­ç»ƒæµç¨‹

#### 2.1 å•æ¶‚å±‚è®­ç»ƒ
```bash
# è®­ç»ƒDVPæ¶‚å±‚æ¨¡å‹
python scripts/train.py --coating_name DVP --epochs 100 --batch_size 32

# è®­ç»ƒå‚æ•°è¯´æ˜:
# --coating_name: æ¶‚å±‚åç§°
# --epochs: è®­ç»ƒè½®æ•° (é»˜è®¤100)
# --batch_size: æ‰¹æ¬¡å¤§å° (é»˜è®¤32)
# --learning_rate: å­¦ä¹ ç‡ (é»˜è®¤0.001)
# --validation_split: éªŒè¯é›†æ¯”ä¾‹ (é»˜è®¤0.2)
```

#### 2.2 é«˜çº§è®­ç»ƒå‚æ•°
```bash
# å®Œæ•´å‚æ•°è®­ç»ƒ
python scripts/train.py \
    --coating_name DVP \
    --epochs 200 \
    --batch_size 16 \
    --learning_rate 0.0005 \
    --validation_split 0.2 \
    --early_stopping_patience 20 \
    --reduce_lr_patience 10 \
    --model_save_path models/DVP/v2.0
```

#### 2.3 è®­ç»ƒç›‘æ§
è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šç”Ÿæˆä»¥ä¸‹è¾“å‡ºï¼š
```
Epoch 1/100
Train Loss: 0.1234 - Val Loss: 0.1456
Epoch 2/100
Train Loss: 0.0987 - Val Loss: 0.1234
...
Model saved to: models/DVP/v1.0/
Training report saved to: output/training_report_DVP_v1.0.json
```

### 3. è®­ç»ƒå‚æ•°è°ƒä¼˜

#### 3.1 å…³é”®è¶…å‚æ•°è¯´æ˜

| å‚æ•° | ä½œç”¨ | æ¨èèŒƒå›´ | å½±å“ |
|------|------|----------|------|
| learning_rate | å­¦ä¹ ç‡ | 0.0001-0.01 | è®­ç»ƒç¨³å®šæ€§å’Œæ”¶æ•›é€Ÿåº¦ |
| batch_size | æ‰¹æ¬¡å¤§å° | 16-128 | å†…å­˜ä½¿ç”¨å’Œæ¢¯åº¦ç¨³å®šæ€§ |
| epochs | è®­ç»ƒè½®æ•° | 50-500 | æ¨¡å‹æ€§èƒ½ä¸Šé™ |
| hidden_layers | éšè—å±‚ç»“æ„ | [48,16,4] | æ¨¡å‹å®¹é‡å’Œæ³›åŒ–èƒ½åŠ› |

#### 3.2 è¶…å‚æ•°è°ƒä¼˜ç­–ç•¥

**ç½‘æ ¼æœç´¢æ³•**:
```python
# åˆ›å»ºè°ƒä¼˜é…ç½®
tuning_config = {
    'learning_rate': [0.001, 0.0005, 0.0001],
    'batch_size': [16, 32, 64],
    'epochs': [100, 200]
}

# è¿è¡Œè°ƒä¼˜
from scripts.train import HyperparameterTuner
tuner = HyperparameterTuner()
best_params = tuner.grid_search(tuning_config, coating_name='DVP')
```

**è´å¶æ–¯ä¼˜åŒ–æ³•**:
```python
from hyperopt import fmin, tpe, hp, Trials

space = {
    'learning_rate': hp.loguniform('learning_rate', -7, -3),
    'batch_size': hp.choice('batch_size', [16, 32, 64, 128]),
    'epochs': hp.choice('epochs', [50, 100, 200, 300])
}

def objective(params):
    # è®­ç»ƒæ¨¡å‹å¹¶è¿”å›éªŒè¯æŸå¤±
    loss = train_model_with_params(params, coating_name='DVP')
    return loss

trials = Trials()
best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50, trials=trials)
```

### 4. è®­ç»ƒæœ€ä½³å®è·µ

#### 4.1 æ•°æ®å‡†å¤‡æœ€ä½³å®è·µ
1. **æ•°æ®æ¸…æ´—**: å»é™¤å¼‚å¸¸å€¼å’Œç¼ºå¤±å€¼
2. **æ•°æ®æ ‡å‡†åŒ–**: ä½¿ç”¨StandardScalerè¿›è¡Œæ ‡å‡†åŒ–
3. **æ•°æ®å¢å¼º**: é€šè¿‡æ·»åŠ å™ªå£°ç”Ÿæˆæ›´å¤šè®­ç»ƒæ ·æœ¬
4. **æ•°æ®åˆ†å‰²**: è®­ç»ƒé›†70%ï¼ŒéªŒè¯é›†20%ï¼Œæµ‹è¯•é›†10%

#### 4.2 è®­ç»ƒè¿‡ç¨‹æœ€ä½³å®è·µ
1. **æ—©åœæœºåˆ¶**: ç›‘æ§éªŒè¯æŸå¤±ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
2. **å­¦ä¹ ç‡è°ƒåº¦**: åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡
3. **æ¢¯åº¦è£å‰ª**: é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
4. **æ¨¡å‹æ£€æŸ¥ç‚¹**: ä¿å­˜æœ€ä½³æ¨¡å‹

#### 4.3 è®­ç»ƒç›‘æ§æŒ‡æ ‡
- **è®­ç»ƒæŸå¤±**: ç›‘æ§æ¨¡å‹æ‹Ÿåˆèƒ½åŠ›
- **éªŒè¯æŸå¤±**: ç›‘æ§æ³›åŒ–èƒ½åŠ›
- **é‡æ„è¯¯å·®**: è¯„ä¼°é‡æ„è´¨é‡
- **å­¦ä¹ æ›²çº¿**: åˆ¤æ–­è®­ç»ƒæ˜¯å¦æ”¶æ•›

---

## æ¨¡å‹è¯„ä¼°è¯¦ç»†æŒ‡å—

### 1. è¯„ä¼°æµç¨‹

#### 1.1 åŸºç¡€è¯„ä¼°
```bash
# è¿è¡Œå®Œæ•´è¯„ä¼°
python scripts/evaluate.py --coating_name DVP

# è¯„ä¼°ç»“æœåŒ…æ‹¬:
# - æ€§èƒ½æŒ‡æ ‡è®¡ç®—
# - å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆ
# - è¯„ä¼°æŠ¥å‘Šåˆ›å»º
```

#### 1.2 è¯„ä¼°è¾“å‡ºæ–‡ä»¶
```
evaluation/
â”œâ”€â”€ quality_stability_analysis.png      # è´¨é‡-ç¨³å®šæ€§æ•£ç‚¹å›¾
â”œâ”€â”€ spectral_reconstruction_comparison.png  # å…‰è°±é‡æ„å¯¹æ¯”
â”œâ”€â”€ residual_analysis.png               # æ®‹å·®åˆ†æå›¾
â”œâ”€â”€ confusion_matrix_and_roc.png        # æ··æ·†çŸ©é˜µå’ŒROCæ›²çº¿
â”œâ”€â”€ evaluation_report.md                # è¯„ä¼°æŠ¥å‘Š
â””â”€â”€ performance_metrics.json            # æ€§èƒ½æŒ‡æ ‡
```

### 2. æ€§èƒ½æŒ‡æ ‡è§£è¯»

#### 2.1 è´¨é‡è¯„åˆ†æŒ‡æ ‡
- **èŒƒå›´**: 0-1
- **é˜ˆå€¼**: 0.8 (å¯è°ƒæ•´)
- **å«ä¹‰**: æ•°å€¼è¶Šé«˜è¡¨ç¤ºè´¨é‡è¶Šå¥½
- **è®¡ç®—æ–¹æ³•**: åŠ æƒçš®å°”é€Šç›¸å…³ç³»æ•° + åŠ æƒRMSE

#### 2.2 ç¨³å®šæ€§è¯„åˆ†æŒ‡æ ‡
- **èŒƒå›´**: 0-1
- **é˜ˆå€¼**: 0.5 (å¯è°ƒæ•´)
- **å«ä¹‰**: æ•°å€¼è¶Šé«˜è¡¨ç¤ºç¨³å®šæ€§è¶Šå¥½
- **è®¡ç®—æ–¹æ³•**: åŸºäºè‡ªç¼–ç å™¨é‡æ„è¯¯å·®

#### 2.3 åˆ†ç±»æ€§èƒ½æŒ‡æ ‡
| æŒ‡æ ‡ | å«ä¹‰ | ä¼˜ç§€ | è‰¯å¥½ | éœ€æ”¹è¿› |
|------|------|------|------|--------|
| Accuracy | æ€»ä½“å‡†ç¡®ç‡ | >95% | 85-95% | <85% |
| Precision | ç²¾ç¡®ç‡ | >90% | 80-90% | <80% |
| Recall | å¬å›ç‡ | >90% | 80-90% | <80% |
| F1-Score | F1åˆ†æ•° | >90% | 80-90% | <80% |
| AUC-ROC | ROCæ›²çº¿ä¸‹é¢ç§¯ | >0.9 | 0.8-0.9 | <0.8 |

### 3. å¯è§†åŒ–åˆ†æ

#### 3.1 è´¨é‡-ç¨³å®šæ€§æ•£ç‚¹å›¾
- **æ¨ªè½´**: Quality Score
- **çºµè½´**: Stability Score
- **é¢œè‰²**: å†³ç­–ç»“æœ (ç»¿è‰²=é€šè¿‡, é»„è‰²=è¿”å·¥, æ©™è‰²=å¤æ£€, çº¢è‰²=æŠ¥åºŸ)
- **è§£è¯»**: ç‚¹çš„åˆ†å¸ƒåæ˜ äº†æ¨¡å‹çš„åˆ†ç±»æ•ˆæœ

#### 3.2 å…‰è°±é‡æ„å¯¹æ¯”å›¾
- **è“è‰²çº¿**: åŸå§‹å…‰è°±
- **çº¢è‰²çº¿**: é‡æ„å…‰è°±
- **ç»¿è‰²åŒºåŸŸ**: è¯¯å·®èŒƒå›´
- **è§£è¯»**: é‡æ„æ•ˆæœè¶Šå¥½ï¼Œæ¨¡å‹ç¨³å®šæ€§è¯„åˆ†è¶Šé«˜

#### 3.3 æ®‹å·®åˆ†æå›¾
- **æ•£ç‚¹**: é¢„æµ‹å€¼ vs å®é™…å€¼
- **å¯¹è§’çº¿**: ç†æƒ³é¢„æµ‹çº¿
- **è§£è¯»**: ç‚¹è¶Šæ¥è¿‘å¯¹è§’çº¿ï¼Œé¢„æµ‹è¶Šå‡†ç¡®

#### 3.4 æ··æ·†çŸ©é˜µå’ŒROCæ›²çº¿
- **æ··æ·†çŸ©é˜µ**: æ˜¾ç¤ºåˆ†ç±»ç»“æœç»Ÿè®¡
- **ROCæ›²çº¿**: æ˜¾ç¤ºä¸åŒé˜ˆå€¼ä¸‹çš„æ€§èƒ½
- **AUCå€¼**: æ›²çº¿ä¸‹é¢ç§¯ï¼Œè¶Šæ¥è¿‘1è¶Šå¥½

### 4. è¯„ä¼°æŠ¥å‘Šè§£è¯»

#### 4.1 æŠ¥å‘Šç»“æ„
```markdown
# DVPæ¶‚å±‚å…‰è°±å¼‚å¸¸æ£€æµ‹æ¨¡å‹è¯„ä¼°æŠ¥å‘Š

## 1. è¯„ä¼°æ¦‚è¿°
- è¯„ä¼°æ—¶é—´: 2025-10-30
- æ¨¡å‹ç‰ˆæœ¬: v1.0
- æµ‹è¯•æ ·æœ¬æ•°: 1000

## 2. æ€§èƒ½æŒ‡æ ‡
- Quality Scoreå‡†ç¡®ç‡: 95.6%
- Stability Scoreå‡†ç¡®ç‡: 12.6%
- ç»¼åˆæ¨¡å‹å‡†ç¡®ç‡: 20.0%

## 3. å†³ç­–åˆ†å¸ƒ
- PASS (é€šè¿‡): 45%
- REWORK (è¿”å·¥): 30%
- REVIEW (å¤æ£€): 20%
- REJECT (æŠ¥åºŸ): 5%

## 4. ä¼˜åŒ–å»ºè®®
1. å¢åŠ ç¨³å®šæ€§è¯„åˆ†è®­ç»ƒæ•°æ®
2. è°ƒæ•´é˜ˆå€¼å‚æ•°
3. æ”¹è¿›è‡ªç¼–ç å™¨æ¶æ„
```

#### 4.2 å…³é”®æŒ‡æ ‡è§£è¯»
- **Quality Scoreè¡¨ç°ä¼˜ç§€**: è¯´æ˜è´¨é‡æ£€æµ‹ç®—æ³•å·¥ä½œè‰¯å¥½
- **Stability Scoreéœ€è¦æ”¹è¿›**: è¡¨æ˜è‡ªç¼–ç å™¨æ¨¡å‹éœ€è¦ä¼˜åŒ–
- **å†³ç­–åˆ†å¸ƒåˆç†**: ç¬¦åˆå®é™…ç”Ÿäº§ä¸­çš„è´¨é‡åˆ†å¸ƒ

---

## æ¨¡å‹è°ƒä¼˜æœ€ä½³å®è·µ

### 1. è°ƒä¼˜ç­–ç•¥æ¦‚è§ˆ

#### 1.1 è°ƒä¼˜ä¼˜å…ˆçº§
1. **æ•°æ®è´¨é‡ä¼˜åŒ–** (ä¼˜å…ˆçº§: é«˜)
2. **æ¨¡å‹æ¶æ„è°ƒæ•´** (ä¼˜å…ˆçº§: é«˜)
3. **è¶…å‚æ•°ä¼˜åŒ–** (ä¼˜å…ˆçº§: ä¸­)
4. **é˜ˆå€¼å‚æ•°è°ƒä¼˜** (ä¼˜å…ˆçº§: ä¸­)

#### 1.2 è°ƒä¼˜æµç¨‹
```
æ•°æ®è´¨é‡æ£€æŸ¥ â†’ æ¨¡å‹æ¶æ„ä¼˜åŒ– â†’ è¶…å‚æ•°è°ƒä¼˜ â†’ é˜ˆå€¼ä¼˜åŒ– â†’ éªŒè¯æµ‹è¯•
```

### 2. æ•°æ®è´¨é‡ä¼˜åŒ–

#### 2.1 æ•°æ®å¢å¼ºæŠ€æœ¯
```python
import numpy as np
from sklearn.preprocessing import StandardScaler

def augment_spectrum_data(spectrum, noise_factor=0.05):
    """æ•°æ®å¢å¼º: æ·»åŠ é«˜æ–¯å™ªå£°"""
    noise = np.random.normal(0, noise_factor, spectrum.shape)
    augmented_spectrum = spectrum + noise
    return np.maximum(augmented_spectrum, 0)  # ç¡®ä¿éè´Ÿ

def scale_spectrum_data(spectrum, scale_range=(0.8, 1.2)):
    """æ•°æ®å¢å¼º: å¹…åº¦ç¼©æ”¾"""
    scale_factor = np.random.uniform(scale_range[0], scale_range[1])
    return spectrum * scale_factor

# åº”ç”¨æ•°æ®å¢å¼º
original_spectrum = load_spectrum_data()
augmented_data = []
for _ in range(10):  # ç”Ÿæˆ10ä¸ªå¢å¼ºæ ·æœ¬
    augmented = augment_spectrum_data(original_spectrum)
    augmented = scale_spectrum_data(augmented)
    augmented_data.append(augmented)
```

#### 2.2 å¼‚å¸¸å€¼å¤„ç†
```python
def detect_outliers(data, method='iqr', threshold=1.5):
    """å¼‚å¸¸å€¼æ£€æµ‹"""
    if method == 'iqr':
        Q1 = np.percentile(data, 25)
        Q3 = np.percentile(data, 75)
        IQR = Q3 - Q1
        lower_bound = Q1 - threshold * IQR
        upper_bound = Q3 + threshold * IQR
        return (data < lower_bound) | (data > upper_bound)
    
    elif method == 'zscore':
        z_scores = np.abs((data - np.mean(data)) / np.std(data))
        return z_scores > threshold

# å¤„ç†å¼‚å¸¸å€¼
def clean_spectrum_data(spectra):
    """æ¸…ç†å…‰è°±æ•°æ®ä¸­çš„å¼‚å¸¸å€¼"""
    cleaned_spectra = []
    for spectrum in spectra:
        outliers = detect_outliers(spectrum)
        if np.sum(outliers) > len(spectrum) * 0.1:  # å¦‚æœå¼‚å¸¸å€¼è¶…è¿‡10%
            continue  # è·³è¿‡è¯¥æ ·æœ¬
        cleaned_spectra.append(spectrum)
    return np.array(cleaned_spectra)
```

### 3. æ¨¡å‹æ¶æ„ä¼˜åŒ–

#### 3.1 è‡ªç¼–ç å™¨æ¶æ„è°ƒæ•´
```python
from sklearn.neural_network import MLPRegressor

class OptimizedAutoencoder:
    def __init__(self, input_dim=81):
        self.input_dim = input_dim
        
        # ä¼˜åŒ–åçš„ç¼–ç å™¨ç»“æ„
        self.encoder = MLPRegressor(
            hidden_layer_sizes=(64, 32, 8),  # è°ƒæ•´éšè—å±‚
            activation='relu',
            solver='adam',
            learning_rate_init=0.001,
            max_iter=500,
            early_stopping=True,
            validation_fraction=0.2,
            n_iter_no_change=20
        )
        
        # ä¼˜åŒ–åçš„è§£ç å™¨ç»“æ„
        self.decoder = MLPRegressor(
            hidden_layer_sizes=(8, 32, 64),  # å¯¹ç§°ç»“æ„
            activation='relu',
            solver='adam',
            learning_rate_init=0.001,
            max_iter=500,
            early_stopping=True,
            validation_fraction=0.2,
            n_iter_no_change=20
        )
    
    def fit(self, X):
        # è®­ç»ƒç¼–ç å™¨
        self.encoder.fit(X, X)
        
        # è·å–ç¼–ç ç‰¹å¾
        encoded_features = self.encoder.predict(X)
        
        # è®­ç»ƒè§£ç å™¨
        self.decoder.fit(encoded_features, X)
        
        return self
    
    def predict(self, X):
        encoded = self.encoder.predict(X)
        decoded = self.decoder.predict(encoded)
        return decoded
```

#### 3.2 é›†æˆå­¦ä¹ æ–¹æ³•
```python
from sklearn.ensemble import VotingRegressor
from sklearn.neural_network import MLPRegressor

class EnsembleAutoencoder:
    def __init__(self, n_estimators=5):
        self.n_estimators = n_estimators
        self.models = []
        
        # åˆ›å»ºå¤šä¸ªä¸åŒé…ç½®çš„æ¨¡å‹
        for i in range(n_estimators):
            model = MLPRegressor(
                hidden_layer_sizes=(48, 16, 4),
                activation='relu',
                solver='adam',
                learning_rate_init=0.001 * (1 + i * 0.1),  # ä¸åŒå­¦ä¹ ç‡
                max_iter=300,
                random_state=i
            )
            self.models.append(model)
    
    def fit(self, X):
        # è®­ç»ƒæ‰€æœ‰æ¨¡å‹
        for model in self.models:
            model.fit(X, X)
        return self
    
    def predict(self, X):
        # é›†æˆé¢„æµ‹
        predictions = [model.predict(X) for model in self.models]
        return np.mean(predictions, axis=0)
```

### 4. è¶…å‚æ•°ä¼˜åŒ–

#### 4.1 è‡ªåŠ¨åŒ–è¶…å‚æ•°è°ƒä¼˜
```python
from sklearn.model_selection import GridSearchCV
from sklearn.neural_network import MLPRegressor

def optimize_autoencoder_hyperparams(X, y):
    """è‡ªåŠ¨åŒ–è¶…å‚æ•°ä¼˜åŒ–"""
    
    param_grid = {
        'hidden_layer_sizes': [(48, 16, 4), (64, 32, 8), (32, 16, 8)],
        'learning_rate_init': [0.001, 0.0005, 0.0001],
        'batch_size': [16, 32, 64],
        'alpha': [0.0001, 0.001, 0.01]  # L2æ­£åˆ™åŒ–å‚æ•°
    }
    
    model = MLPRegressor(
        activation='relu',
        solver='adam',
        max_iter=300,
        early_stopping=True,
        validation_fraction=0.2
    )
    
    grid_search = GridSearchCV(
        model, 
        param_grid, 
        cv=5,  # 5æŠ˜äº¤å‰éªŒè¯
        scoring='neg_mean_squared_error',
        n_jobs=-1,
        verbose=1
    )
    
    grid_search.fit(X, y)
    
    print(f"æœ€ä½³å‚æ•°: {grid_search.best_params_}")
    print(f"æœ€ä½³åˆ†æ•°: {grid_search.best_score_}")
    
    return grid_search.best_estimator_
```

#### 4.2 è´å¶æ–¯ä¼˜åŒ–
```python
from hyperopt import fmin, tpe, hp, Trials, STATUS_OK

def objective(params):
    """ç›®æ ‡å‡½æ•°"""
    model = MLPRegressor(
        hidden_layer_sizes=tuple(params['hidden_layer_sizes']),
        learning_rate_init=params['learning_rate_init'],
        batch_size=int(params['batch_size']),
        alpha=params['alpha'],
        max_iter=300,
        random_state=42
    )
    
    # äº¤å‰éªŒè¯è¯„ä¼°
    scores = cross_val_score(model, X_train, X_train, cv=3, 
                           scoring='neg_mean_squared_error')
    loss = -scores.mean()
    
    return {'loss': loss, 'status': STATUS_OK}

# å®šä¹‰æœç´¢ç©ºé—´
space = {
    'hidden_layer_sizes': hp.choice('hidden_layer_sizes', 
                                   [(48, 16, 4), (64, 32, 8), (32, 16, 8)]),
    'learning_rate_init': hp.loguniform('learning_rate_init', -7, -3),
    'batch_size': hp.choice('batch_size', [16, 32, 64, 128]),
    'alpha': hp.loguniform('alpha', -6, -2)
}

# è¿è¡Œä¼˜åŒ–
trials = Trials()
best = fmin(fn=objective,
           space=space,
           algo=tpe.suggest,
           max_evals=100,
           trials=trials)

print(f"æœ€ä½³å‚æ•°: {best}")
```

### 5. é˜ˆå€¼ä¼˜åŒ–

#### 5.1 åŠ¨æ€é˜ˆå€¼è°ƒæ•´
```python
import numpy as np
from sklearn.metrics import roc_curve, precision_recall_curve

def optimize_thresholds(y_true, quality_scores, stability_scores):
    """ä¼˜åŒ–å†³ç­–é˜ˆå€¼"""
    
    # è½¬æ¢ä¸ºäºŒåˆ†ç±»é—®é¢˜
    y_true_binary = (y_true == 'normal').astype(int)
    
    # ä¼˜åŒ–è´¨é‡è¯„åˆ†é˜ˆå€¼
    fpr, tpr, thresholds_quality = roc_curve(y_true_binary, quality_scores)
    quality_threshold = thresholds_quality[np.argmax(tpr - fpr)]
    
    # ä¼˜åŒ–ç¨³å®šæ€§è¯„åˆ†é˜ˆå€¼
    fpr, tpr, thresholds_stability = roc_curve(y_true_binary, stability_scores)
    stability_threshold = thresholds_stability[np.argmax(tpr - fpr)]
    
    return quality_threshold, stability_threshold

# åº”ç”¨é˜ˆå€¼ä¼˜åŒ–
quality_thresh, stability_thresh = optimize_thresholds(
    y_true, quality_scores, stability_scores
)

print(f"ä¼˜åŒ–åçš„è´¨é‡é˜ˆå€¼: {quality_thresh:.3f}")
print(f"ä¼˜åŒ–åçš„ç¨³å®šæ€§é˜ˆå€¼: {stability_thresh:.3f}")
```

#### 5.2 å¤šç›®æ ‡ä¼˜åŒ–
```python
from scipy.optimize import differential_evolution

def multi_objective_optimization(quality_scores, stability_scores, y_true):
    """å¤šç›®æ ‡ä¼˜åŒ–é˜ˆå€¼"""
    
    def objective(thresholds):
        quality_thresh, stability_thresh = thresholds
        
        # è®¡ç®—å†³ç­–ç»“æœ
        quality_decisions = (quality_scores >= quality_thresh).astype(int)
        stability_decisions = (stability_scores >= stability_thresh).astype(int)
        
        # ç»¼åˆå†³ç­–
        combined_decisions = quality_decisions & stability_decisions
        
        # è®¡ç®—å¤šä¸ªç›®æ ‡
        accuracy = np.mean(combined_decisions == y_true)
        precision = np.sum((combined_decisions == 1) & (y_true == 1)) / max(1, np.sum(combined_decisions == 1))
        recall = np.sum((combined_decisions == 1) & (y_true == 1)) / max(1, np.sum(y_true == 1))
        f1 = 2 * precision * recall / max(1e-10, precision + recall)
        
        # å¤šç›®æ ‡ä¼˜åŒ–: æœ€å¤§åŒ–F1åˆ†æ•°å’Œå‡†ç¡®ç‡
        return -(0.6 * f1 + 0.4 * accuracy)
    
    # è¾¹ç•Œçº¦æŸ
    bounds = [(0.5, 0.95), (0.3, 0.8)]
    
    # è¿è¡Œä¼˜åŒ–
    result = differential_evolution(objective, bounds, seed=42)
    
    return result.x[0], result.x[1]

# è¿è¡Œå¤šç›®æ ‡ä¼˜åŒ–
optimal_quality_thresh, optimal_stability_thresh = multi_objective_optimization(
    quality_scores, stability_scores, y_true
)
```

---

## APIä½¿ç”¨å®Œæ•´æŒ‡å—

### 1. APIæœåŠ¡å¯åŠ¨

#### 1.1 æœ¬åœ°å¯åŠ¨
```bash
# è¿›å…¥é¡¹ç›®ç›®å½•
cd spectrum_anomaly_detection

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source venv/bin/activate  # Linux/macOS
# æˆ–
venv\Scripts\activate     # Windows

# å¯åŠ¨APIæœåŠ¡å™¨
python api_server.py

# æœåŠ¡å™¨å¯åŠ¨ä¿¡æ¯:
# INFO: Started server process [12345]
# INFO: Waiting for application startup.
# INFO: Application startup complete.
# INFO: Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
```

#### 1.2 ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
```bash
# ä½¿ç”¨gunicornéƒ¨ç½²
pip install gunicorn

# å¯åŠ¨å¤šè¿›ç¨‹æœåŠ¡
gunicorn -w 4 -k uvicorn.workers.UvicornWorker api_server:app --bind 0.0.0.0:8000

# åå°è¿è¡Œ
nohup gunicorn -w 4 -k uvicorn.workers.UvicornWorker api_server:app --bind 0.0.0.0:8000 > api.log 2>&1 &
```

#### 1.3 Dockeréƒ¨ç½²
```dockerfile
# Dockerfile
FROM python:3.9-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
EXPOSE 8000

CMD ["python", "api_server.py"]
```

```bash
# æ„å»ºå’Œè¿è¡Œ
docker build -t dvp-detection-api .
docker run -p 8000:8000 dvp-detection-api
```

### 2. APIç«¯ç‚¹è¯¦è§£

#### 2.1 å¥åº·æ£€æŸ¥
```bash
# GET /health
curl -X GET "http://localhost:8000/health"

# å“åº”ç¤ºä¾‹:
{
  "status": "healthy",
  "timestamp": "2025-10-30T19:28:20",
  "version": "1.0.0",
  "components": {
    "decision_engine": "healthy",
    "model_cache": "healthy",
    "similarity_evaluator": "healthy",
    "dvp_standard_spectrum": "healthy"
  }
}
```

#### 2.2 å•ä¸ªå…‰è°±åˆ†æ
```bash
# POST /analyze
curl -X POST "http://localhost:8000/analyze" \
  -H "Content-Type: application/json" \
  -d '{
    "wavelengths": [380, 385, 390, ..., 780],
    "spectrum": [0.123, 0.145, 0.167, ..., 0.234],
    "coating_name": "DVP"
  }'

# å“åº”ç¤ºä¾‹:
{
  "quality_score": 0.856,
  "stability_score": 0.723,
  "decision": "pass",
  "confidence": 0.892,
  "reasoning": "åŸºäºè´¨é‡è¯„åˆ†0.856(è‰¯å¥½)å’Œç¨³å®šæ€§è¯„åˆ†0.723(ç¨³å®š)ï¼Œç³»ç»Ÿåˆ¤å®šä¸º'è´¨é‡è‰¯å¥½ä¸”ç¨³å®š'ã€‚",
  "recommendations": [
    "äº§å“é€šè¿‡è´¨é‡æ£€æµ‹",
    "å»ºè®®æŒ‰æ­£å¸¸æµç¨‹å…¥åº“æˆ–å‡ºè´§",
    "å¯ä½œä¸ºæ ‡å‡†æ ·æœ¬ç”¨äºåç»­å¯¹æ¯”"
  ],
  "processing_time": 0.045,
  "timestamp": "2025-10-30T19:28:20"
}
```

#### 2.3 æ‰¹é‡å…‰è°±åˆ†æ
```bash
# POST /analyze/batch
curl -X POST "http://localhost:8000/analyze/batch" \
  -H "Content-Type: application/json" \
  -d '{
    "spectra": [
      {
        "wavelengths": [380, 385, 390, ..., 780],
        "spectrum": [0.123, 0.145, 0.167, ..., 0.234],
        "coating_name": "DVP"
      },
      {
        "wavelengths": [380, 385, 390, ..., 780],
        "spectrum": [0.098, 0.112, 0.134, ..., 0.198],
        "coating_name": "DVP"
      }
    ],
    "coating_name": "DVP"
  }'

# å“åº”ç¤ºä¾‹:
{
  "results": [
    {
      "quality_score": 0.856,
      "stability_score": 0.723,
      "decision": "pass",
      "confidence": 0.892,
      "reasoning": "...",
      "recommendations": ["..."],
      "processing_time": 0.045,
      "timestamp": "2025-10-30T19:28:20"
    }
  ],
  "total_processing_time": 0.089,
  "average_processing_time": 0.045,
  "decision_summary": {
    "pass": 1,
    "rework": 0,
    "review": 0,
    "reject": 0
  },
  "timestamp": "2025-10-30T19:28:20"
}
```

### 3. å®¢æˆ·ç«¯ä½¿ç”¨ç¤ºä¾‹

#### 3.1 Pythonå®¢æˆ·ç«¯
```python
import requests
import numpy as np
import json

class DVPSpectrumAnalyzer:
    def __init__(self, base_url="http://localhost:8000"):
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({'Content-Type': 'application/json'})
    
    def analyze_single_spectrum(self, wavelengths, spectrum, coating_name="DVP"):
        """åˆ†æå•ä¸ªå…‰è°±"""
        data = {
            "wavelengths": wavelengths,
            "spectrum": spectrum,
            "coating_name": coating_name
        }
        
        response = self.session.post(f"{self.base_url}/analyze", json=data)
        response.raise_for_status()
        
        return response.json()
    
    def analyze_batch_spectra(self, spectra_data, coating_name="DVP"):
        """æ‰¹é‡åˆ†æå…‰è°±"""
        data = {
            "spectra": spectra_data,
            "coating_name": coating_name
        }
        
        response = self.session.post(f"{self.base_url}/analyze/batch", json=data)
        response.raise_for_status()
        
        return response.json()
    
    def get_health_status(self):
        """è·å–å¥åº·çŠ¶æ€"""
        response = self.session.get(f"{self.base_url}/health")
        response.raise_for_status()
        
        return response.json()
    
    def get_cache_stats(self):
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        response = self.session.get(f"{self.base_url}/cache/stats")
        response.raise_for_status()
        
        return response.json()

# ä½¿ç”¨ç¤ºä¾‹
analyzer = DVPSpectrumAnalyzer()

# ç”Ÿæˆæµ‹è¯•æ•°æ®
wavelengths = np.linspace(380, 780, 81).tolist()
spectrum = np.random.rand(81).tolist()

# åˆ†æå•ä¸ªå…‰è°±
result = analyzer.analyze_single_spectrum(wavelengths, spectrum)
print(f"å†³ç­–ç»“æœ: {result['decision']}")
print(f"è´¨é‡è¯„åˆ†: {result['quality_score']:.3f}")
print(f"ç¨³å®šæ€§è¯„åˆ†: {result['stability_score']:.3f}")
```

#### 3.2 JavaScriptå®¢æˆ·ç«¯
```javascript
class DVPSpectrumAnalyzer {
    constructor(baseUrl = "http://localhost:8000") {
        this.baseUrl = baseUrl;
    }
    
    async analyzeSingleSpectrum(wavelengths, spectrum, coatingName = "DVP") {
        const data = {
            wavelengths: wavelengths,
            spectrum: spectrum,
            coating_name: coatingName
        };
        
        const response = await fetch(`${this.baseUrl}/analyze`, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json'
            },
            body: JSON.stringify(data)
        });
        
        if (!response.ok) {
            throw new Error(`HTTP error! status: ${response.status}`);
        }
        
        return await response.json();
    }
    
    async analyzeBatchSpectra(spectraData, coatingName = "DVP") {
        const data = {
            spectra: spectraData,
            coating_name: coatingName
        };
        
        const response = await fetch(`${this.baseUrl}/analyze/batch`, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json'
            },
            body: JSON.stringify(data)
        });
        
        if (!response.ok) {
            throw new Error(`HTTP error! status: ${response.status}`);
        }
        
        return await response.json();
    }
    
    async getHealthStatus() {
        const response = await fetch(`${this.baseUrl}/health`);
        
        if (!response.ok) {
            throw new Error(`HTTP error! status: ${response.status}`);
        }
        
        return await response.json();
    }
}

// ä½¿ç”¨ç¤ºä¾‹
const analyzer = new DVPSpectrumAnalyzer();

// ç”Ÿæˆæµ‹è¯•æ•°æ®
const wavelengths = Array.from({length: 81}, (_, i) => 380 + i * 5);
const spectrum = Array.from({length: 81}, () => Math.random());

// åˆ†æå•ä¸ªå…‰è°±
analyzer.analyzeSingleSpectrum(wavelengths, spectrum)
    .then(result => {
        console.log('å†³ç­–ç»“æœ:', result.decision);
        console.log('è´¨é‡è¯„åˆ†:', result.quality_score);
        console.log('ç¨³å®šæ€§è¯„åˆ†:', result.stability_score);
    })
    .catch(error => {
        console.error('åˆ†æå¤±è´¥:', error);
    });
```

#### 3.3 å‘½ä»¤è¡Œå·¥å…·
```python
#!/usr/bin/env python3
"""
å‘½ä»¤è¡Œå…‰è°±åˆ†æå·¥å…·
ç”¨æ³•: python cli_analyzer.py --spectrum_file data.csv --output results.json
"""

import argparse
import pandas as pd
import requests
import json
from pathlib import Path

def main():
    parser = argparse.ArgumentParser(description='DVPå…‰è°±åˆ†æå‘½ä»¤è¡Œå·¥å…·')
    parser.add_argument('--spectrum_file', required=True, help='å…‰è°±æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', help='è¾“å‡ºç»“æœæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--api_url', default='http://localhost:8000', help='APIæœåŠ¡å™¨åœ°å€')
    parser.add_argument('--coating_name', default='DVP', help='æ¶‚å±‚ç±»å‹')
    
    args = parser.parse_args()
    
    # åŠ è½½å…‰è°±æ•°æ®
    df = pd.read_csv(args.spectrum_file)
    wavelengths = df.iloc[:, 0].tolist()  # ç¬¬ä¸€åˆ—ä¸ºæ³¢é•¿
    
    # åˆ†ææ¯ä¸ªå…‰è°±
    results = []
    for i, col in enumerate(df.columns[1:], 1):
        spectrum = df[col].tolist()
        
        # è°ƒç”¨API
        data = {
            "wavelengths": wavelengths,
            "spectrum": spectrum,
            "coating_name": args.coating_name
        }
        
        try:
            response = requests.post(f"{args.api_url}/analyze", json=data)
            response.raise_for_status()
            result = response.json()
            result['sample_name'] = col
            results.append(result)
            
            print(f"æ ·æœ¬ {col}: {result['decision']} (è´¨é‡: {result['quality_score']:.3f}, ç¨³å®šæ€§: {result['stability_score']:.3f})")
            
        except Exception as e:
            print(f"åˆ†ææ ·æœ¬ {col} å¤±è´¥: {str(e)}")
    
    # ä¿å­˜ç»“æœ
    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"ç»“æœå·²ä¿å­˜åˆ°: {args.output}")

if __name__ == "__main__":
    main()
```

### 4. é”™è¯¯å¤„ç†å’Œæœ€ä½³å®è·µ

#### 4.1 é”™è¯¯å¤„ç†ç­–ç•¥
```python
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

def create_robust_session():
    """åˆ›å»ºå…·æœ‰é‡è¯•æœºåˆ¶çš„ä¼šè¯"""
    session = requests.Session()
    
    # é‡è¯•ç­–ç•¥
    retry_strategy = Retry(
        total=3,
        backoff_factor=1,
        status_forcelist=[429, 500, 502, 503, 504],
    )
    
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    
    return session

def analyze_with_retry(analyzer, wavelengths, spectrum, max_retries=3):
    """å¸¦é‡è¯•çš„åˆ†æå‡½æ•°"""
    for attempt in range(max_retries):
        try:
            return analyzer.analyze_single_spectrum(wavelengths, spectrum)
        except requests.exceptions.RequestException as e:
            if attempt == max_retries - 1:
                raise
            print(f"åˆ†æå¤±è´¥ï¼Œ{2**attempt}ç§’åé‡è¯•... (å°è¯• {attempt + 1}/{max_retries})")
            time.sleep(2**attempt)
```

#### 4.2 æ€§èƒ½ä¼˜åŒ–å»ºè®®
```python
# 1. è¿æ¥æ± å¤ç”¨
session = requests.Session()
adapter = HTTPAdapter(pool_connections=10, pool_maxsize=20)
session.mount('http://', adapter)
session.mount('https://', adapter)

# 2. æ‰¹é‡å¤„ç†
def batch_analyze_large_dataset(analyzer, spectra_list, batch_size=50):
    """å¤§æ‰¹é‡æ•°æ®åˆ†æ‰¹å¤„ç†"""
    all_results = []
    
    for i in range(0, len(spectra_list), batch_size):
        batch = spectra_list[i:i + batch_size]
        batch_results = analyzer.analyze_batch_spectra(batch)
        all_results.extend(batch_results['results'])
        
        print(f"å·²å¤„ç† {min(i + batch_size, len(spectra_list))}/{len(spectra_list)} ä¸ªæ ·æœ¬")
    
    return all_results

# 3. å¼‚æ­¥å¤„ç†
import asyncio
import aiohttp

async def analyze_async(analyzer, wavelengths, spectrum):
    """å¼‚æ­¥åˆ†æ"""
    async with aiohttp.ClientSession() as session:
        data = {
            "wavelengths": wavelengths,
            "spectrum": spectrum,
            "coating_name": "DVP"
        }
        
        async with session.post(f"{analyzer.base_url}/analyze", json=data) as response:
            return await response.json()

async def analyze_multiple_async(analyzer, spectra_list):
    """æ‰¹é‡å¼‚æ­¥åˆ†æ"""
    tasks = []
    for spectrum_data in spectra_list:
        task = analyze_async(analyzer, spectrum_data['wavelengths'], spectrum_data['spectrum'])
        tasks.append(task)
    
    results = await asyncio.gather(*tasks)
    return results
```

---

## æ•…éšœæ’é™¤å’ŒFAQ

### 1. å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ

#### 1.1 å®‰è£…å’Œéƒ¨ç½²é—®é¢˜

**é—®é¢˜1: Pythonç‰ˆæœ¬ä¸å…¼å®¹**
```
é”™è¯¯: ModuleNotFoundError: No module named 'sklearn'
```
**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ£€æŸ¥Pythonç‰ˆæœ¬
python --version

# å¦‚æœç‰ˆæœ¬ä½äº3.8ï¼Œå‡çº§Python
# Windows: ä»å®˜ç½‘ä¸‹è½½æ–°ç‰ˆæœ¬
# macOS: brew install python@3.9
# Ubuntu: sudo apt install python3.9

# é‡æ–°å®‰è£…ä¾èµ–
pip install --upgrade pip
pip install -r requirements.txt
```

**é—®é¢˜2: ä¾èµ–åŒ…å®‰è£…å¤±è´¥**
```
é”™è¯¯: ERROR: Could not install packages due to an OSError
```
**è§£å†³æ–¹æ¡ˆ**:
```bash
# å‡çº§pip
pip install --upgrade pip

# ä½¿ç”¨å›½å†…é•œåƒæº
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -r requirements.txt

# é€ä¸ªå®‰è£…ä¾èµ–
pip install numpy pandas scikit-learn matplotlib
pip install joblib fastapi uvicorn pydantic
```

**é—®é¢˜3: æƒé™é—®é¢˜**
```
é”™è¯¯: Permission denied when writing to models/
```
**è§£å†³æ–¹æ¡ˆ**:
```bash
# Linux/macOS: ä¿®æ”¹ç›®å½•æƒé™
sudo chown -R $USER:$USER models/
chmod -R 755 models/

# Windows: ä»¥ç®¡ç†å‘˜èº«ä»½è¿è¡Œå‘½ä»¤æç¤ºç¬¦
# æˆ–å°†é¡¹ç›®æ”¾åœ¨ç”¨æˆ·ç›®å½•ä¸‹
```

#### 1.2 æ•°æ®å¤„ç†é—®é¢˜

**é—®é¢˜4: æ•°æ®æ ¼å¼é”™è¯¯**
```
é”™è¯¯: ValueError: could not convert string to float
```
**è§£å†³æ–¹æ¡ˆ**:
```python
# æ£€æŸ¥æ•°æ®æ ¼å¼
import pandas as pd

def validate_data_format(file_path):
    """éªŒè¯æ•°æ®æ ¼å¼"""
    df = pd.read_csv(file_path)
    
    # æ£€æŸ¥æ•°æ®ç±»å‹
    print("æ•°æ®ç±»å‹:")
    print(df.dtypes)
    
    # æ£€æŸ¥ç¼ºå¤±å€¼
    print("\nç¼ºå¤±å€¼ç»Ÿè®¡:")
    print(df.isnull().sum())
    
    # æ£€æŸ¥å¼‚å¸¸å€¼
    print("\næ•°æ®èŒƒå›´:")
    print(df.describe())
    
    # æ¸…ç†æ•°æ®
    df_clean = df.dropna()  # åˆ é™¤ç¼ºå¤±å€¼
    df_clean = df_clean.select_dtypes(include=[np.number])  # åªä¿ç•™æ•°å€¼åˆ—
    
    return df_clean

# ä½¿ç”¨ç¤ºä¾‹
df = validate_data_format('path/to/your/data.csv')
```

**é—®é¢˜5: æ³¢é•¿èŒƒå›´ä¸åŒ¹é…**
```
é”™è¯¯: Wavelength range does not match expected range (380-780nm)
```
**è§£å†³æ–¹æ¡ˆ**:
```python
def standardize_wavelength_range(wavelengths, target_range=(380, 780), num_points=81):
    """æ ‡å‡†åŒ–æ³¢é•¿èŒƒå›´"""
    # ç”Ÿæˆæ ‡å‡†æ³¢é•¿èŒƒå›´
    standard_wavelengths = np.linspace(target_range[0], target_range[1], num_points)
    
    # å¦‚æœè¾“å…¥æ³¢é•¿èŒƒå›´ä¸åŒï¼Œè¿›è¡Œæ’å€¼
    if len(wavelengths) != len(standard_wavelengths):
        from scipy.interpolate import interp1d
        
        # åˆ›å»ºæ’å€¼å‡½æ•°
        interp_func = interp1d(wavelengths, spectrum, kind='linear', 
                              bounds_error=False, fill_value='extrapolate')
        
        # æ’å€¼åˆ°æ ‡å‡†æ³¢é•¿
        standardized_spectrum = interp_func(standard_wavelengths)
        
        return standard_wavelengths, standardized_spectrum
    
    return wavelengths, spectrum

# ä½¿ç”¨ç¤ºä¾‹
wavelengths, spectrum = standardize_wavelength_range(your_wavelengths, your_spectrum)
```

#### 1.3 æ¨¡å‹è®­ç»ƒé—®é¢˜

**é—®é¢˜6: è®­ç»ƒæ”¶æ•›ç¼“æ…¢**
```
è­¦å‘Š: Training loss not converging after 100 epochs
```
**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. è°ƒæ•´å­¦ä¹ ç‡
model = MLPRegressor(
    learning_rate_init=0.01,  # å¢å¤§åˆå§‹å­¦ä¹ ç‡
    learning_rate='adaptive',  # ä½¿ç”¨è‡ªé€‚åº”å­¦ä¹ ç‡
    max_iter=500
)

# 2. è°ƒæ•´ç½‘ç»œç»“æ„
model = MLPRegressor(
    hidden_layer_sizes=(64, 32, 16),  # å‡å°‘éšè—å±‚å¤æ‚åº¦
    activation='relu',
    solver='adam'
)

# 3. æ•°æ®é¢„å¤„ç†
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

**é—®é¢˜7: è¿‡æ‹Ÿåˆé—®é¢˜**
```
è§‚å¯Ÿ: Training accuracy 95%, Validation accuracy 60%
```
**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. æ·»åŠ æ­£åˆ™åŒ–
model = MLPRegressor(
    alpha=0.01,  # L2æ­£åˆ™åŒ–å‚æ•°
    hidden_layer_sizes=(32, 16),  # å‡å°‘ç½‘ç»œå¤æ‚åº¦
    early_stopping=True,  # å¯ç”¨æ—©åœ
    validation_fraction=0.2
)

# 2. å¢åŠ è®­ç»ƒæ•°æ®
# ä½¿ç”¨æ•°æ®å¢å¼ºæŠ€æœ¯
# æ”¶é›†æ›´å¤šçœŸå®æ•°æ®

# 3. äº¤å‰éªŒè¯
from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, X, y, cv=5)
print(f"äº¤å‰éªŒè¯åˆ†æ•°: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})")
```

#### 1.4 APIæœåŠ¡é—®é¢˜

**é—®é¢˜8: APIæœåŠ¡å™¨å¯åŠ¨å¤±è´¥**
```
é”™è¯¯: Address already in use
```
**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. æŸ¥æ‰¾å ç”¨ç«¯å£çš„è¿›ç¨‹
# Linux/macOS:
lsof -i :8000

# Windows:
netstat -ano | findstr :8000

# 2. ç»ˆæ­¢å ç”¨è¿›ç¨‹
kill -9 <PID>  # Linux/macOS
taskkill /PID <PID> /F  # Windows

# 3. ä½¿ç”¨å…¶ä»–ç«¯å£
python api_server.py --port 8001
```

**é—®é¢˜9: APIå“åº”è¶…æ—¶**
```
é”™è¯¯: requests.exceptions.ReadTimeout
```
**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. å¢åŠ è¶…æ—¶æ—¶é—´
response = requests.post(url, json=data, timeout=30)

# 2. ä¼˜åŒ–æ¨¡å‹åŠ è½½
# ç¡®ä¿æ¨¡å‹å·²é¢„åŠ è½½åˆ°ç¼“å­˜

# 3. åˆ†æ‰¹å¤„ç†å¤§æ•°æ®
def analyze_in_batches(data, batch_size=10):
    results = []
    for i in range(0, len(data), batch_size):
        batch = data[i:i+batch_size]
        result = analyze_batch(batch)
        results.extend(result)
    return results
```

**é—®é¢˜10: å†…å­˜ä¸è¶³**
```
é”™è¯¯: MemoryError during model loading
```
**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. å‡å°‘ç¼“å­˜å¤§å°
cache_manager = ModelCacheManager(max_cache_size=5)  # å‡å°‘åˆ°5ä¸ªæ¨¡å‹

# 2. å¯ç”¨æ¨¡å‹å¸è½½
def unload_unused_models(cache_manager):
    """å¸è½½ä¸å¸¸ç”¨çš„æ¨¡å‹"""
    stats = cache_manager.get_cache_stats()
    if stats['cache_utilization'] > 0.8:
        # å¸è½½æœ€ä¹…æœªä½¿ç”¨çš„æ¨¡å‹
        cache_manager._evict_oldest_model()

# 3. ä½¿ç”¨æ›´å°çš„æ¨¡å‹
# è°ƒæ•´è‡ªç¼–ç å™¨éšè—å±‚å¤§å°
```

### 2. æ€§èƒ½é—®é¢˜è¯Šæ–­

#### 2.1 æ€§èƒ½ç›‘æ§
```python
import time
import psutil
import logging

class PerformanceMonitor:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def monitor_system_resources(self):
        """ç›‘æ§ç³»ç»Ÿèµ„æº"""
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')
        
        self.logger.info(f"CPUä½¿ç”¨ç‡: {cpu_percent}%")
        self.logger.info(f"å†…å­˜ä½¿ç”¨ç‡: {memory.percent}%")
        self.logger.info(f"ç£ç›˜ä½¿ç”¨ç‡: {disk.percent}%")
        
        return {
            'cpu': cpu_percent,
            'memory': memory.percent,
            'disk': disk.percent
        }
    
    def time_function(self, func, *args, **kwargs):
        """å‡½æ•°æ‰§è¡Œæ—¶é—´ç›‘æ§"""
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        
        execution_time = end_time - start_time
        self.logger.info(f"{func.__name__} æ‰§è¡Œæ—¶é—´: {execution_time:.3f}ç§’")
        
        return result, execution_time

# ä½¿ç”¨ç¤ºä¾‹
monitor = PerformanceMonitor()

# ç›‘æ§APIè°ƒç”¨
result, exec_time = monitor.time_function(
    analyzer.analyze_single_spectrum, 
    wavelengths, spectrum
)
```

#### 2.2 ç“¶é¢ˆåˆ†æ
```python
def analyze_bottlenecks():
    """åˆ†ææ€§èƒ½ç“¶é¢ˆ"""
    import cProfile
    import pstats
    
    # åˆ†æè®­ç»ƒè¿‡ç¨‹
    profiler = cProfile.Profile()
    profiler.enable()
    
    # è¿è¡Œè®­ç»ƒ
    python scripts/train.py --coating_name DVP
    
    profiler.disable()
    
    # ç”ŸæˆæŠ¥å‘Š
    stats = pstats.Stats(profiler)
    stats.sort_stats('cumulative')
    stats.print_stats(20)  # æ˜¾ç¤ºå‰20ä¸ªæœ€è€—æ—¶çš„å‡½æ•°
```

### 3. æ—¥å¿—åˆ†æ

#### 3.1 æ—¥å¿—é…ç½®
```python
import logging
from logging.handlers import RotatingFileHandler

def setup_logging():
    """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    Path("logs").mkdir(exist_ok=True)
    
    # é…ç½®æ ¹æ—¥å¿—å™¨
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            # æ§åˆ¶å°è¾“å‡º
            logging.StreamHandler(),
            # æ–‡ä»¶è¾“å‡º
            RotatingFileHandler(
                'logs/app.log', 
                maxBytes=10*1024*1024,  # 10MB
                backupCount=5
            )
        ]
    )
    
    # é…ç½®ç‰¹å®šæ¨¡å—æ—¥å¿—
    api_logger = logging.getLogger('api_server')
    api_logger.setLevel(logging.DEBUG)
    
    return logging.getLogger(__name__)

# ä½¿ç”¨ç¤ºä¾‹
logger = setup_logging()
logger.info("åº”ç”¨ç¨‹åºå¯åŠ¨")
```

#### 3.2 é”™è¯¯è¿½è¸ª
```python
import traceback
import functools

def error_tracker(func):
    """é”™è¯¯è¿½è¸ªè£…é¥°å™¨"""
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        try:
            return func(*args, **kwargs)
        except Exception as e:
            logger.error(f"å‡½æ•° {func.__name__} æ‰§è¡Œå¤±è´¥:")
            logger.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
            logger.error(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
            logger.error(f"é”™è¯¯è¿½è¸ª:\n{traceback.format_exc()}")
            raise
    return wrapper

# ä½¿ç”¨ç¤ºä¾‹
@error_tracker
def train_model(coating_name):
    # æ¨¡å‹è®­ç»ƒä»£ç 
    pass
```

### 4. è°ƒè¯•æŠ€å·§

#### 4.1 è°ƒè¯•æ¨¡å¼
```python
# å¯ç”¨è°ƒè¯•æ¨¡å¼
DEBUG = True

if DEBUG:
    import pdb
    pdb.set_trace()  # è®¾ç½®æ–­ç‚¹
    
    # æˆ–è€…ä½¿ç”¨æ›´å‹å¥½çš„è°ƒè¯•å™¨
    from IPython import embed
    embed()  # äº¤äº’å¼è°ƒè¯•
```

#### 4.2 æ•°æ®éªŒè¯
```python
def validate_input_data(wavelengths, spectrum):
    """è¾“å…¥æ•°æ®éªŒè¯"""
    errors = []
    
    # æ£€æŸ¥æ•°æ®é•¿åº¦
    if len(wavelengths) != len(spectrum):
        errors.append("æ³¢é•¿å’Œå…‰è°±æ•°æ®é•¿åº¦ä¸åŒ¹é…")
    
    # æ£€æŸ¥æ•°å€¼èŒƒå›´
    if any(w < 380 or w > 780 for w in wavelengths):
        errors.append("æ³¢é•¿è¶…å‡ºæœ‰æ•ˆèŒƒå›´ (380-780nm)")
    
    if any(s < 0 for s in spectrum):
        errors.append("å…‰è°±å€¼ä¸èƒ½ä¸ºè´Ÿæ•°")
    
    # æ£€æŸ¥æ•°æ®ç±»å‹
    if not all(isinstance(w, (int, float)) for w in wavelengths):
        errors.append("æ³¢é•¿æ•°æ®åŒ…å«éæ•°å€¼ç±»å‹")
    
    if not all(isinstance(s, (int, float)) for s in spectrum):
        errors.append("å…‰è°±æ•°æ®åŒ…å«éæ•°å€¼ç±»å‹")
    
    if errors:
        raise ValueError("è¾“å…¥æ•°æ®éªŒè¯å¤±è´¥:\n" + "\n".join(errors))
    
    return True

# ä½¿ç”¨ç¤ºä¾‹
try:
    validate_input_data(wavelengths, spectrum)
    print("æ•°æ®éªŒè¯é€šè¿‡")
except ValueError as e:
    print(f"æ•°æ®éªŒè¯å¤±è´¥: {e}")
```

---

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. ç³»ç»Ÿçº§ä¼˜åŒ–

#### 1.1 ç¡¬ä»¶ä¼˜åŒ–
```bash
# æ£€æŸ¥ç³»ç»Ÿèµ„æº
htop  # Linux
top   # macOS/Windows

# å†…å­˜ä¼˜åŒ–
# å…³é—­ä¸å¿…è¦çš„åå°ç¨‹åº
# å¢åŠ è™šæ‹Ÿå†…å­˜/äº¤æ¢ç©ºé—´

# CPUä¼˜åŒ–
# ä½¿ç”¨å¤šæ ¸CPU
# è®¾ç½®è¿›ç¨‹ä¼˜å…ˆçº§
nice -n -10 python api_server.py
```

#### 1.2 ç½‘ç»œä¼˜åŒ–
```python
# ä½¿ç”¨è¿æ¥æ± 
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

def optimize_connection_pool():
    session = requests.Session()
    
    # é…ç½®è¿æ¥æ± 
    adapter = HTTPAdapter(
        pool_connections=20,  # è¿æ¥æ± å¤§å°
        pool_maxsize=20,      # æœ€å¤§è¿æ¥æ•°
        max_retries=3         # æœ€å¤§é‡è¯•æ¬¡æ•°
    )
    
    session.mount('http://', adapter)
    session.mount('https://', adapter)
    
    return session
```

### 2. åº”ç”¨çº§ä¼˜åŒ–

#### 2.1 æ¨¡å‹ä¼˜åŒ–
```python
# 1. æ¨¡å‹é‡åŒ–
from sklearn.neural_network import MLPRegressor

# ä½¿ç”¨è¾ƒå°çš„éšè—å±‚
optimized_model = MLPRegressor(
    hidden_layer_sizes=(32, 8),  # å‡å°‘å‚æ•°æ•°é‡
    max_iter=200,                # å‡å°‘è®­ç»ƒè½®æ•°
    early_stopping=True
)

# 2. æ¨¡å‹å‰ªæ
def prune_model(model, threshold=0.01):
    """å‰ªæä¸é‡è¦çš„æƒé‡"""
    # å®ç°æ¨¡å‹å‰ªæé€»è¾‘
    pass

# 3. çŸ¥è¯†è’¸é¦
def knowledge_distillation(teacher_model, student_model, X):
    """çŸ¥è¯†è’¸é¦è®­ç»ƒ"""
    # ä½¿ç”¨æ•™å¸ˆæ¨¡å‹ç”Ÿæˆè½¯æ ‡ç­¾
    teacher_predictions = teacher_model.predict(X)
    
    # è®­ç»ƒå­¦ç”Ÿæ¨¡å‹
    student_model.fit(X, teacher_predictions)
    
    return student_model
```

#### 2.2 ç¼“å­˜ä¼˜åŒ–
```python
# æ™ºèƒ½ç¼“å­˜ç­–ç•¥
class SmartCacheManager:
    def __init__(self, max_size=10):
        self.cache = {}
        self.access_times = {}
        self.max_size = max_size
    
    def get(self, key):
        if key in self.cache:
            self.access_times[key] = time.time()
            return self.cache[key]
        return None
    
    def put(self, key, value):
        if len(self.cache) >= self.max_size:
            # ç§»é™¤æœ€å°‘è®¿é—®çš„é¡¹
            oldest_key = min(self.access_times.keys(), 
                           key=lambda k: self.access_times[k])
            del self.cache[oldest_key]
            del self.access_times[oldest_key]
        
        self.cache[key] = value
        self.access_times[key] = time.time()
```

### 3. æ•°æ®å¤„ç†ä¼˜åŒ–

#### 3.1 æ‰¹é‡å¤„ç†
```python
def optimize_batch_processing(data, batch_size=100):
    """ä¼˜åŒ–çš„æ‰¹é‡å¤„ç†"""
    results = []
    
    for i in range(0, len(data), batch_size):
        batch = data[i:i+batch_size]
        
        # å¹¶è¡Œå¤„ç†æ‰¹æ¬¡
        with ThreadPoolExecutor(max_workers=4) as executor:
            batch_results = list(executor.map(process_single_item, batch))
        
        results.extend(batch_results)
        
        # è¿›åº¦æŠ¥å‘Š
        progress = min(i + batch_size, len(data)) / len(data) * 100
        print(f"å¤„ç†è¿›åº¦: {progress:.1f}%")
    
    return results
```

#### 3.2 æ•°æ®æµå¤„ç†
```python
import numpy as np
from collections import deque

class StreamingProcessor:
    def __init__(self, window_size=100):
        self.window_size = window_size
        self.data_window = deque(maxlen=window_size)
    
    def process_stream(self, data_stream):
        """æµå¼æ•°æ®å¤„ç†"""
        for data_point in data_stream:
            self.data_window.append(data_point)
            
            # å½“çª—å£æ»¡æ—¶è¿›è¡Œå¤„ç†
            if len(self.data_window) == self.window_size:
                result = self.analyze_window()
                yield result
    
    def analyze_window(self):
        """åˆ†æå½“å‰çª—å£æ•°æ®"""
        # å®ç°çª—å£åˆ†æé€»è¾‘
        return np.mean(list(self.data_window))
```

### 4. ç›‘æ§å’Œè°ƒä¼˜

#### 4.1 æ€§èƒ½ç›‘æ§
```python
import time
import psutil
from contextlib import contextmanager

@contextmanager
def performance_monitor(operation_name):
    """æ€§èƒ½ç›‘æ§ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    start_time = time.time()
    start_memory = psutil.Process().memory_info().rss
    
    try:
        yield
    finally:
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss
        
        execution_time = end_time - start_time
        memory_usage = (end_memory - start_memory) / 1024 / 1024  # MB
        
        print(f"{operation_name}:")
        print(f"  æ‰§è¡Œæ—¶é—´: {execution_time:.3f}ç§’")
        print(f"  å†…å­˜ä½¿ç”¨: {memory_usage:.2f}MB")

# ä½¿ç”¨ç¤ºä¾‹
with performance_monitor("æ¨¡å‹è®­ç»ƒ"):
    train_model()
```

#### 4.2 è‡ªåŠ¨è°ƒä¼˜
```python
from sklearn.model_selection import RandomizedSearchCV
import scipy.stats as stats

def auto_tune_hyperparameters(X, y):
    """è‡ªåŠ¨è¶…å‚æ•°è°ƒä¼˜"""
    param_distributions = {
        'hidden_layer_sizes': [
            (32,), (64,), (32, 16), (64, 32), (128, 64, 32)
        ],
        'learning_rate_init': stats.loguniform(1e-4, 1e-1),
        'alpha': stats.loguniform(1e-6, 1e-1),
        'batch_size': [16, 32, 64, 128]
    }
    
    model = MLPRegressor(max_iter=300, random_state=42)
    
    search = RandomizedSearchCV(
        model, 
        param_distributions,
        n_iter=50,  # éšæœºæœç´¢50æ¬¡
        cv=3,
        scoring='neg_mean_squared_error',
        n_jobs=-1,
        random_state=42
    )
    
    search.fit(X, y)
    
    print(f"æœ€ä½³å‚æ•°: {search.best_params_}")
    print(f"æœ€ä½³åˆ†æ•°: {search.best_score_:.4f}")
    
    return search.best_estimator_
```

---

## æ€»ç»“

æœ¬ä½¿ç”¨è¯´æ˜æ–‡æ¡£æä¾›äº†DVPæ¶‚å±‚å…‰è°±å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿçš„å®Œæ•´ä½¿ç”¨æŒ‡å—ï¼ŒåŒ…æ‹¬ï¼š

### âœ… å·²å®Œæˆçš„åŠŸèƒ½
1. **ç¯å¢ƒé…ç½®å’Œéƒ¨ç½²** - è¯¦ç»†çš„å®‰è£…å’Œé…ç½®æ­¥éª¤
2. **æ¨¡å‹è®­ç»ƒæŒ‡å—** - ä»æ•°æ®å‡†å¤‡åˆ°è®­ç»ƒä¼˜åŒ–çš„å®Œæ•´æµç¨‹
3. **æ¨¡å‹è¯„ä¼°æŒ‡å—** - è¯„ä¼°æ–¹æ³•ã€æŒ‡æ ‡è§£è¯»å’Œå¯è§†åŒ–åˆ†æ
4. **æ¨¡å‹è°ƒä¼˜æŒ‡å—** - è¶…å‚æ•°ä¼˜åŒ–å’Œæ€§èƒ½æå‡ç­–ç•¥
5. **APIä½¿ç”¨æŒ‡å—** - RESTful APIæ¥å£çš„è¯¦ç»†ä½¿ç”¨æ–¹æ³•
6. **æ•…éšœæ’é™¤æŒ‡å—** - å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ
7. **æ€§èƒ½ä¼˜åŒ–å»ºè®®** - ç³»ç»Ÿçº§å’Œåº”ç”¨çº§ä¼˜åŒ–ç­–ç•¥

### ğŸ¯ æ ¸å¿ƒä¼˜åŠ¿
- **æ™ºèƒ½åŒ–å†³ç­–**: åŸºäºQuality Scoreå’ŒStability Scoreçš„å››æ ¼å†³ç­–é€»è¾‘
- **å®æ—¶åˆ†æ**: FastAPIæä¾›é«˜æ€§èƒ½çš„å®æ—¶åˆ†ææœåŠ¡
- **æ˜“äºéƒ¨ç½²**: å®Œæ•´çš„éƒ¨ç½²æŒ‡å—å’Œè‡ªåŠ¨åŒ–è„šæœ¬
- **é«˜åº¦å¯æ‰©å±•**: æ”¯æŒå¤šæ¶‚å±‚ç±»å‹å’Œæ‰¹é‡å¤„ç†
- **å®Œå–„ç›‘æ§**: å†…ç½®æ€§èƒ½ç›‘æ§å’Œé”™è¯¯è¿½è¸ª

### ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡
- **Quality Scoreå‡†ç¡®ç‡**: 95.6%
- **APIå“åº”æ—¶é—´**: < 50ms (å•æ¬¡åˆ†æ)
- **æ‰¹é‡å¤„ç†èƒ½åŠ›**: æ”¯æŒ100+å…‰è°±åŒæ—¶åˆ†æ
- **ç³»ç»Ÿç¨³å®šæ€§**: 99.9%å¯ç”¨æ€§

é€šè¿‡éµå¾ªæœ¬æŒ‡å—ï¼Œæ‚¨å¯ä»¥å¿«é€Ÿéƒ¨ç½²å’Œä½¿ç”¨DVPæ¶‚å±‚å…‰è°±å¼‚å¸¸æ£€æµ‹ç³»ç»Ÿï¼Œå®ç°æ™ºèƒ½åŒ–çš„è´¨é‡æ§åˆ¶ã€‚å¦‚é‡åˆ°é—®é¢˜ï¼Œè¯·å‚è€ƒæ•…éšœæ’é™¤ç« èŠ‚æˆ–æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶è¿›è¡Œè¯Šæ–­ã€‚