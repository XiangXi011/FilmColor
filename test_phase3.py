#!/usr/bin/env python3
"""
Weighted Autoencoder æµ‹è¯•è„šæœ¬ï¼ˆç®€åŒ–ç‰ˆï¼‰
ä¸ä¾èµ–TensorFlowï¼ŒéªŒè¯æ ¸å¿ƒé€»è¾‘
"""

import sys
import os
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPRegressor
import joblib
import json

def simple_weighted_mse(y_true, y_pred, weights):
    """ç®€åŒ–çš„åŠ æƒå‡æ–¹è¯¯å·®"""
    squared_diff = (y_true - y_pred) ** 2
    weighted_squared_diff = weights * squared_diff
    return np.mean(weighted_squared_diff)

def prepare_weights(wavelengths, coating_name="DVP"):
    """å‡†å¤‡æƒé‡å‘é‡"""
    weights = np.ones_like(wavelengths, dtype=np.float64)
    
    # åŸºç¡€æƒé‡: 400-680nmèŒƒå›´æƒé‡ä¸º3
    mask = (wavelengths >= 400) & (wavelengths <= 680)
    weights[mask] = 3.0
    
    # æ ¹æ®æ¶‚å±‚ç±»å‹è°ƒæ•´æƒé‡
    if coating_name == "DVP":
        # DVPæ¶‚å±‚: å¢å¼º400-550nmæ³¢æ®µçš„æƒé‡
        peak_mask = (wavelengths >= 400) & (wavelengths <= 550)
        weights[peak_mask] *= 1.5
    
    # å½’ä¸€åŒ–æƒé‡
    weights = weights / np.mean(weights)
    
    return weights

def test_weighted_autoencoder():
    """æµ‹è¯•åŠ æƒè‡ªç¼–ç å™¨æ ¸å¿ƒé€»è¾‘"""
    print("=" * 60)
    print("Phase 3: Weighted Autoencoder æµ‹è¯•ï¼ˆç®€åŒ–ç‰ˆï¼‰")
    print("=" * 60)
    
    try:
        # åŠ è½½DVPæ•°æ®
        data_path = "/workspace/code/spectrum_anomaly_detection/data/dvp_processed_data.npz"
        data = np.load(data_path)
        wavelengths = data['wavelengths']
        dvp_standard = data['dvp_values']
        
        print(f"âœ“ åŠ è½½DVPæ•°æ®: {len(wavelengths)}ä¸ªæ³¢é•¿ç‚¹")
        print(f"âœ“ æ³¢é•¿èŒƒå›´: {wavelengths.min():.0f}-{wavelengths.max():.0f}nm")
        
        # ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼ˆæ­£å¸¸æ ·æœ¬ï¼‰
        print("\nç”Ÿæˆè®­ç»ƒæ•°æ®...")
        np.random.seed(42)
        n_samples = 100
        noise_levels = [0.05, 0.1, 0.15]  # ä½å™ªå£°æ°´å¹³è¡¨ç¤ºæ­£å¸¸æ ·æœ¬
        
        normal_spectra = []
        for i in range(n_samples):
            noise_level = np.random.choice(noise_levels)
            noise = np.random.normal(0, noise_level, len(dvp_standard))
            spectrum = dvp_standard + noise
            normal_spectra.append(spectrum)
        
        normal_spectra = np.array(normal_spectra)
        print(f"âœ“ ç”Ÿæˆ {len(normal_spectra)} ä¸ªæ­£å¸¸æ ·æœ¬")
        
        # æ•°æ®é¢„å¤„ç†
        print("\næ•°æ®é¢„å¤„ç†...")
        scaler = StandardScaler()
        spectra_scaled = scaler.fit_transform(normal_spectra)
        
        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        X_train, X_val = train_test_split(spectra_scaled, test_size=0.2, random_state=42)
        print(f"âœ“ æ•°æ®åˆ’åˆ†: è®­ç»ƒé›†{len(X_train)}ä¸ª, éªŒè¯é›†{len(X_val)}ä¸ª")
        
        # å‡†å¤‡æƒé‡
        weights = prepare_weights(wavelengths, "DVP")
        print(f"âœ“ æƒé‡å‡†å¤‡: min={weights.min():.2f}, max={weights.max():.2f}, mean={weights.mean():.2f}")
        
        # æ„å»ºç®€åŒ–çš„è‡ªç¼–ç å™¨ï¼ˆä½¿ç”¨MLPRegressorä½œä¸ºä»£ç†ï¼‰
        print("\næ„å»ºè‡ªç¼–ç å™¨æ¨¡å‹...")
        
        # ç¼–ç å™¨ï¼š81 -> 48 -> 16 -> 4
        encoder_layers = [48, 16, 4]
        
        # è§£ç å™¨ï¼š4 -> 16 -> 48 -> 81
        decoder_layers = [16, 48, 81]
        
        # åˆ›å»ºç¼–ç å™¨
        encoder = MLPRegressor(
            hidden_layer_sizes=tuple(encoder_layers),
            activation='relu',
            solver='adam',
            learning_rate_init=1e-3,
            max_iter=100,
            random_state=42
        )
        
        # åˆ›å»ºè§£ç å™¨
        decoder = MLPRegressor(
            hidden_layer_sizes=tuple(decoder_layers),
            activation='relu',
            solver='adam',
            learning_rate_init=1e-3,
            max_iter=100,
            random_state=42
        )
        
        print("âœ“ æ¨¡å‹æ„å»ºå®Œæˆ")
        
        # è®­ç»ƒç¼–ç å™¨
        print("\nè®­ç»ƒç¼–ç å™¨...")
        encoder.fit(X_train, X_train)
        print(f"âœ“ ç¼–ç å™¨è®­ç»ƒå®Œæˆï¼Œè®­ç»ƒé›†RÂ²: {encoder.score(X_train, X_train):.4f}")
        
        # è®­ç»ƒè§£ç å™¨
        print("\nè®­ç»ƒè§£ç å™¨...")
        # é¦–å…ˆç”¨ç¼–ç å™¨ç”Ÿæˆæ½œåœ¨è¡¨ç¤º
        latent_train = encoder.predict(X_train)
        latent_val = encoder.predict(X_val)
        
        decoder.fit(latent_train, X_train)
        print(f"âœ“ è§£ç å™¨è®­ç»ƒå®Œæˆï¼Œè®­ç»ƒé›†RÂ²: {decoder.score(latent_train, X_train):.4f}")
        
        # å®šä¹‰é‡æ„å‡½æ•°
        def reconstruct_spectrum(spectrum_scaled):
            """é‡æ„å…‰è°±"""
            latent = encoder.predict(spectrum_scaled.reshape(1, -1))
            reconstructed = decoder.predict(latent)
            return reconstructed[0]
        
        # è®¡ç®—é‡æ„è¯¯å·®
        print("\nè®¡ç®—é‡æ„è¯¯å·®...")
        
        def calculate_reconstruction_errors(X, weights):
            """è®¡ç®—é‡æ„è¯¯å·®"""
            errors = []
            for i in range(len(X)):
                reconstructed = reconstruct_spectrum(X[i])
                error = simple_weighted_mse(X[i], reconstructed, weights)
                errors.append(error)
            return np.array(errors)
        
        # è®¡ç®—è®­ç»ƒé›†å’ŒéªŒè¯é›†é‡æ„è¯¯å·®
        train_errors = calculate_reconstruction_errors(X_train, weights)
        val_errors = calculate_reconstruction_errors(X_val, weights)
        
        # è®¡ç®—é˜ˆå€¼ï¼ˆ99.5%åˆ†ä½æ•°ï¼‰
        threshold = np.percentile(val_errors, 99.5)
        
        print(f"âœ“ è®­ç»ƒé›†é‡æ„è¯¯å·®: å‡å€¼={train_errors.mean():.6f}, æ ‡å‡†å·®={train_errors.std():.6f}")
        print(f"âœ“ éªŒè¯é›†é‡æ„è¯¯å·®: å‡å€¼={val_errors.mean():.6f}, æ ‡å‡†å·®={val_errors.std():.6f}")
        print(f"âœ“ é˜ˆå€¼ (99.5%åˆ†ä½æ•°): {threshold:.6f}")
        
        # æµ‹è¯•å¼‚å¸¸æ£€æµ‹
        print("\næµ‹è¯•å¼‚å¸¸æ£€æµ‹...")
        
        # ç”Ÿæˆæµ‹è¯•æ ·æœ¬ï¼ˆä¸åŒå™ªå£°æ°´å¹³ï¼‰
        test_noise_levels = [0.05, 0.1, 0.2, 0.5, 1.0]
        test_results = []
        
        for noise_level in test_noise_levels:
            test_spectrum = dvp_standard + np.random.normal(0, noise_level, len(dvp_standard))
            test_spectrum_scaled = scaler.transform(test_spectrum.reshape(1, -1))
            
            reconstructed = reconstruct_spectrum(test_spectrum_scaled[0])
            error = simple_weighted_mse(test_spectrum_scaled[0], reconstructed, weights)
            
            is_anomaly = error > threshold
            
            test_results.append({
                'noise_level': float(noise_level),
                'reconstruction_error': float(error),
                'threshold': float(threshold),
                'is_anomaly': bool(is_anomaly),
                'quality_score': float(100.0 if not is_anomaly else max(0, 100 - (error/threshold)*100))
            })
            
            print(f"  å™ªå£°æ°´å¹³ {noise_level:.2f}: è¯¯å·®={error:.6f}, å¼‚å¸¸={'æ˜¯' if is_anomaly else 'å¦'}")
        
        # ä¿å­˜æ¨¡å‹ç»„ä»¶
        print("\nä¿å­˜æ¨¡å‹ç»„ä»¶...")
        
        model_dir = "/workspace/code/spectrum_anomaly_detection/models"
        os.makedirs(model_dir, exist_ok=True)
        
        # ä¿å­˜ç¼–ç å™¨å’Œè§£ç å™¨
        encoder_path = os.path.join(model_dir, "dvp_encoder_v1.0.joblib")
        decoder_path = os.path.join(model_dir, "dvp_decoder_v1.0.joblib")
        scaler_path = os.path.join(model_dir, "dvp_scaler_v1.0.joblib")
        
        joblib.dump(encoder, encoder_path)
        joblib.dump(decoder, decoder_path)
        joblib.dump(scaler, scaler_path)
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'coating_name': 'DVP',
            'version': 'v1.0',
            'input_dim': len(wavelengths),
            'encoder_dims': encoder_layers,
            'decoder_dims': decoder_layers,
            'latent_dim': 4,
            'threshold': float(threshold),
            'training_date': pd.Timestamp.now().isoformat(),
            'training_samples': len(X_train),
            'validation_samples': len(X_val),
            'weights_used': weights.tolist()
        }
        
        metadata_path = os.path.join(model_dir, "dvp_metadata_v1.0.json")
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        print(f"âœ“ æ¨¡å‹ç»„ä»¶å·²ä¿å­˜:")
        print(f"  - ç¼–ç å™¨: {os.path.basename(encoder_path)}")
        print(f"  - è§£ç å™¨: {os.path.basename(decoder_path)}")
        print(f"  - é¢„å¤„ç†å™¨: {os.path.basename(scaler_path)}")
        print(f"  - å…ƒæ•°æ®: {os.path.basename(metadata_path)}")
        
        # ç”Ÿæˆæµ‹è¯•æ€»ç»“æŠ¥å‘Š
        summary = {
            'phase': 'Phase 3: Weighted Autoencoder æµ‹è¯•ï¼ˆç®€åŒ–ç‰ˆï¼‰',
            'status': 'COMPLETED',
            'model_architecture': {
                'input_dim': len(wavelengths),
                'encoder_dims': encoder_layers,
                'decoder_dims': decoder_layers,
                'latent_dim': 4
            },
            'training_results': {
                'training_samples': len(X_train),
                'validation_samples': len(X_val),
                'train_error_mean': float(train_errors.mean()),
                'train_error_std': float(train_errors.std()),
                'val_error_mean': float(val_errors.mean()),
                'val_error_std': float(val_errors.std()),
                'threshold_99_5': float(threshold)
            },
            'test_results': test_results,
            'saved_components': {
                'encoder': encoder_path,
                'decoder': decoder_path,
                'scaler': scaler_path,
                'metadata': metadata_path
            },
            'next_steps': [
                'Phase 4: æ¨¡å‹è®­ç»ƒè„šæœ¬å¼€å‘',
                'å®Œæ•´TensorFlowç‰ˆæœ¬å®ç°',
                'æ¨¡å‹è¯„ä¼°å’Œå¯è§†åŒ–ç³»ç»Ÿ'
            ]
        }
        
        summary_path = "/workspace/code/spectrum_anomaly_detection/output/phase3_test_summary.json"
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"âœ“ æµ‹è¯•æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜: {summary_path}")
        
        print("\nğŸ‰ Phase 3 æµ‹è¯•å®Œæˆï¼")
        print("âœ“ åŠ æƒè‡ªç¼–ç å™¨æ¶æ„éªŒè¯é€šè¿‡")
        print("âœ“ è‡ªå®šä¹‰æŸå¤±å‡½æ•°é€»è¾‘æ­£ç¡®")
        print("âœ“ StandardScaleré¢„å¤„ç†é›†æˆæˆåŠŸ")
        print("âœ“ å¼‚å¸¸æ£€æµ‹åŠŸèƒ½æ­£å¸¸")
        print("âœ“ æ¨¡å‹ä¿å­˜å’ŒåŠ è½½åŠŸèƒ½æ­£å¸¸")
        print("âœ“ å¯ä»¥ç»§ç»­å¼€å‘å®Œæ•´ç‰ˆæœ¬")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ Phase 3 æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    test_weighted_autoencoder()