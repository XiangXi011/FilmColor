#!/usr/bin/env python3
"""
æ¨¡å‹è®­ç»ƒè„šæœ¬ - train.py
å®Œæ•´çš„DVPæ¶‚å±‚ç±»å‹å…‰è°±å¼‚å¸¸æ£€æµ‹æ¨¡å‹è®­ç»ƒæµç¨‹

åŠŸèƒ½ï¼š
- æ”¯æŒå‘½ä»¤è¡Œå‚æ•°æŒ‡å®šcoating_name
- æ•´åˆSimilarityEvaluatorå’ŒWeightedAutoencoder
- å®ç°æ¨¡å‹ä¿å­˜åŠŸèƒ½(.h5, .pkl, .json)
- é˜ˆå€¼è®¡ç®—(99.5%åˆ†ä½æ•°)
- å¤šäº§å“ç±»å‹è®­ç»ƒæ”¯æŒ
- å®Œæ•´çš„è®­ç»ƒæ—¥å¿—å’ŒæŠ¥å‘Š
"""

import os
import sys
import argparse
import json
import numpy as np
import pandas as pd
from pathlib import Path
import logging
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/workspace/code/spectrum_anomaly_detection')

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from algorithms.similarity_evaluator import SimilarityEvaluator
# from models.weighted_autoencoder import WeightedAutoencoder  # æš‚æ—¶æ³¨é‡Šæ‰TensorFlowç‰ˆæœ¬

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class SpectrumAnomalyTrainer:
    """
    å…‰è°±å¼‚å¸¸æ£€æµ‹æ¨¡å‹è®­ç»ƒå™¨
    
    æ•´åˆQuality Scoreå’ŒStability Scoreçš„è®­ç»ƒæµç¨‹
    """
    
    def __init__(self, coating_name: str = "DVP", version: str = "v1.0"):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            coating_name: æ¶‚å±‚åç§°
            version: æ¨¡å‹ç‰ˆæœ¬å·
        """
        self.coating_name = coating_name
        self.version = version
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.similarity_evaluator = SimilarityEvaluator(coating_name)
        # self.autoencoder = WeightedAutoencoder(input_dim=81, coating_name=coating_name)  # æš‚æ—¶æ³¨é‡Šæ‰TensorFlowç‰ˆæœ¬
        
        # è®­ç»ƒé…ç½®
        self.config = {
            'training_samples': 200,
            'validation_split': 0.2,
            'noise_levels': [0.05, 0.1, 0.15, 0.2],
            'quality_threshold': 85.0,  # Quality Scoreé˜ˆå€¼
            'stability_threshold_percentile': 99.5,  # Stability Scoreé˜ˆå€¼ç™¾åˆ†ä½
            'random_seed': 42
        }
        
        logger.info(f"SpectrumAnomalyTrainer åˆå§‹åŒ–å®Œæˆ [{coating_name} {version}]")
    
    def _prepare_weights(self, wavelengths: np.ndarray, coating_name: str) -> np.ndarray:
        """
        å‡†å¤‡æƒé‡å‘é‡ï¼ˆä¸WeightedAutoencoderä¸­çš„æ–¹æ³•ç›¸åŒï¼‰
        
        Args:
            wavelengths: æ³¢é•¿æ•°ç»„
            coating_name: æ¶‚å±‚åç§°
            
        Returns:
            np.ndarray: æƒé‡å‘é‡
        """
        weights = np.ones_like(wavelengths, dtype=np.float64)
        
        # åŸºç¡€æƒé‡: 400-680nmèŒƒå›´æƒé‡ä¸º3
        mask = (wavelengths >= 400) & (wavelengths <= 680)
        weights[mask] = 3.0
        
        # æ ¹æ®æ¶‚å±‚ç±»å‹è°ƒæ•´æƒé‡
        if coating_name == "DVP":
            # DVPæ¶‚å±‚: å¢å¼º400-550nmæ³¢æ®µçš„æƒé‡
            peak_mask = (wavelengths >= 400) & (wavelengths <= 550)
            weights[peak_mask] *= 1.5
        
        # å½’ä¸€åŒ–æƒé‡
        weights = weights / np.mean(weights)
        
        return weights
    def _prepare_weights(self, wavelengths: np.ndarray, coating_name: str) -> np.ndarray:
        """
        å‡†å¤‡æƒé‡å‘é‡ï¼ˆä¸WeightedAutoencoderä¸­çš„æ–¹æ³•ç›¸åŒï¼‰
        
        Args:
            wavelengths: æ³¢é•¿æ•°ç»„
            coating_name: æ¶‚å±‚åç§°
            
        Returns:
            np.ndarray: æƒé‡å‘é‡
        """
        weights = np.ones_like(wavelengths, dtype=np.float64)
        
        # åŸºç¡€æƒé‡: 400-680nmèŒƒå›´æƒé‡ä¸º3
        mask = (wavelengths >= 400) & (wavelengths <= 680)
        weights[mask] = 3.0
        
        # æ ¹æ®æ¶‚å±‚ç±»å‹è°ƒæ•´æƒé‡
        if coating_name == "DVP":
            # DVPæ¶‚å±‚: å¢å¼º400-550nmæ³¢æ®µçš„æƒé‡
            peak_mask = (wavelengths >= 400) & (wavelengths <= 550)
            weights[peak_mask] *= 1.5
        
        # å½’ä¸€åŒ–æƒé‡
        weights = weights / np.mean(weights)
        
        return weights
    def load_standard_curve(self) -> tuple:
        """
        åŠ è½½æ ‡å‡†æ›²çº¿æ•°æ®
        
        Returns:
            tuple: (æ³¢é•¿æ•°ç»„, æ ‡å‡†å…‰è°±æ•°ç»„)
        """
        data_path = "/workspace/code/spectrum_anomaly_detection/data/dvp_processed_data.npz"
        
        if not os.path.exists(data_path):
            raise FileNotFoundError(f"æ ‡å‡†æ›²çº¿æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        
        data = np.load(data_path)
        wavelengths = data['wavelengths']
        
        # æ ¹æ®æ¶‚å±‚åç§°é€‰æ‹©å¯¹åº”çš„æ ‡å‡†å…‰è°±
        if self.coating_name == "DVP":
            standard_spectrum = data['dvp_values']
        else:
            # å¦‚æœæ˜¯å…¶ä»–æ¶‚å±‚ï¼Œéœ€è¦åŠ è½½å¯¹åº”çš„æ ‡å‡†æ›²çº¿
            # è¿™é‡Œæš‚æ—¶ä½¿ç”¨DVPä½œä¸ºç¤ºä¾‹
            standard_spectrum = data['dvp_values']
            logger.warning(f"æ¶‚å±‚ {self.coating_name} ä½¿ç”¨DVPæ ‡å‡†æ›²çº¿ä½œä¸ºç¤ºä¾‹")
        
        logger.info(f"æ ‡å‡†æ›²çº¿åŠ è½½å®Œæˆ: {len(wavelengths)}ä¸ªæ³¢é•¿ç‚¹")
        return wavelengths, standard_spectrum
    
    def generate_training_data(self, standard_spectrum: np.ndarray, 
                             wavelengths: np.ndarray) -> tuple:
        """
        ç”Ÿæˆè®­ç»ƒæ•°æ®
        
        Args:
            standard_spectrum: æ ‡å‡†å…‰è°±
            wavelengths: æ³¢é•¿æ•°ç»„
            
        Returns:
            tuple: (è®­ç»ƒå…‰è°±, éªŒè¯å…‰è°±, æ ‡ç­¾)
        """
        np.random.seed(self.config['random_seed'])
        
        # ç”Ÿæˆæ­£å¸¸æ ·æœ¬ï¼ˆä½å™ªå£°ï¼‰
        normal_spectra = []
        normal_labels = []
        
        n_samples = self.config['training_samples']
        noise_levels = self.config['noise_levels']
        
        for i in range(n_samples):
            # éšæœºé€‰æ‹©å™ªå£°æ°´å¹³
            noise_level = np.random.choice(noise_levels)
            
            # ç”Ÿæˆå™ªå£°
            noise = np.random.normal(0, noise_level, len(standard_spectrum))
            
            # ç”Ÿæˆå…‰è°±
            spectrum = standard_spectrum + noise
            
            normal_spectra.append(spectrum)
            normal_labels.append(1)  # 1è¡¨ç¤ºæ­£å¸¸
        
        # ç”Ÿæˆä¸€äº›å¼‚å¸¸æ ·æœ¬ï¼ˆé«˜å™ªå£°ï¼‰ç”¨äºéªŒè¯
        anomaly_spectra = []
        anomaly_labels = []
        
        for i in range(50):  # ç”Ÿæˆ50ä¸ªå¼‚å¸¸æ ·æœ¬
            # é«˜å™ªå£°æ°´å¹³
            noise_level = np.random.uniform(0.5, 1.5)
            
            # ç”Ÿæˆå™ªå£°
            noise = np.random.normal(0, noise_level, len(standard_spectrum))
            
            # ç”Ÿæˆå…‰è°±
            spectrum = standard_spectrum + noise
            
            anomaly_spectra.append(spectrum)
            anomaly_labels.append(0)  # 0è¡¨ç¤ºå¼‚å¸¸
        
        # åˆå¹¶æ•°æ®
        all_spectra = np.array(normal_spectra + anomaly_spectra)
        all_labels = np.array(normal_labels + anomaly_labels)
        
        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        from sklearn.model_selection import train_test_split
        
        X_train, X_val, y_train, y_val = train_test_split(
            all_spectra, all_labels, 
            test_size=self.config['validation_split'], 
            random_state=self.config['random_seed'],
            stratify=all_labels
        )
        
        logger.info(f"è®­ç»ƒæ•°æ®ç”Ÿæˆå®Œæˆ:")
        logger.info(f"  - è®­ç»ƒé›†: {len(X_train)}ä¸ªæ ·æœ¬ (æ­£å¸¸{y_train.sum()}, å¼‚å¸¸{len(y_train)-y_train.sum()})")
        logger.info(f"  - éªŒè¯é›†: {len(X_val)}ä¸ªæ ·æœ¬ (æ­£å¸¸{y_val.sum()}, å¼‚å¸¸{len(y_val)-y_val.sum()})")
        
        return X_train, X_val, y_train, y_val
    
    def train_similarity_evaluator(self, X_train: np.ndarray, X_val: np.ndarray,
                                 standard_spectrum: np.ndarray, wavelengths: np.ndarray) -> dict:
        """
        è®­ç»ƒSimilarityEvaluatorï¼ˆå®é™…ä¸Šä¸éœ€è¦è®­ç»ƒï¼Œåªæ˜¯é…ç½®ï¼‰
        
        Args:
            X_train: è®­ç»ƒå…‰è°±
            X_val: éªŒè¯å…‰è°±
            standard_spectrum: æ ‡å‡†å…‰è°±
            wavelengths: æ³¢é•¿æ•°ç»„
            
        Returns:
            dict: è¯„ä¼°ç»“æœ
        """
        logger.info("é…ç½®SimilarityEvaluator...")
        
        # å¯¹è®­ç»ƒé›†å’ŒéªŒè¯é›†è¿›è¡ŒQuality Scoreè¯„ä¼°
        train_results = self.similarity_evaluator.batch_evaluate(
            X_train, standard_spectrum, wavelengths, self.coating_name
        )
        
        val_results = self.similarity_evaluator.batch_evaluate(
            X_val, standard_spectrum, wavelengths, self.coating_name
        )
        
        # è®¡ç®—Quality Scoreé˜ˆå€¼
        quality_threshold = self.config['quality_threshold']
        
        # ç»Ÿè®¡æ­£å¸¸æ ·æœ¬çš„Quality Scoreåˆ†å¸ƒ
        normal_train_scores = train_results[train_results['spectrum_id'] < len(X_train)//2]['similarity_score_percent']
        normal_val_scores = val_results[val_results['spectrum_id'] < len(X_val)//2]['similarity_score_percent']
        
        all_normal_scores = np.concatenate([normal_train_scores, normal_val_scores])
        
        evaluator_result = {
            'training_scores': train_results.to_dict('records'),
            'validation_scores': val_results.to_dict('records'),
            'quality_threshold': quality_threshold,
            'normal_scores_stats': {
                'mean': float(all_normal_scores.mean()),
                'std': float(all_normal_scores.std()),
                'min': float(all_normal_scores.min()),
                'max': float(all_normal_scores.max()),
                'percentile_5': float(np.percentile(all_normal_scores, 5)),
                'percentile_95': float(np.percentile(all_normal_scores, 95))
            }
        }
        
        logger.info(f"SimilarityEvaluatoré…ç½®å®Œæˆ:")
        logger.info(f"  - Quality Scoreé˜ˆå€¼: {quality_threshold}%")
        logger.info(f"  - æ­£å¸¸æ ·æœ¬å¹³å‡åˆ†æ•°: {evaluator_result['normal_scores_stats']['mean']:.2f}%")
        
        return evaluator_result
    
    def train_autoencoder(self, X_train: np.ndarray, X_val: np.ndarray, 
                         wavelengths: np.ndarray) -> dict:
        """
        è®­ç»ƒWeightedAutoencoder
        
        Args:
            X_train: è®­ç»ƒå…‰è°±
            X_val: éªŒè¯å…‰è°±
            wavelengths: æ³¢é•¿æ•°ç»„
            
        Returns:
            dict: è®­ç»ƒç»“æœ
        """
        logger.info("å¼€å§‹è®­ç»ƒWeightedAutoencoder...")
        
        # åªä½¿ç”¨æ­£å¸¸æ ·æœ¬è®­ç»ƒè‡ªç¼–ç å™¨
        normal_train = X_train[:len(X_train)//2]  # å‡è®¾å‰ä¸€åŠæ˜¯æ­£å¸¸æ ·æœ¬
        
        # ä½¿ç”¨ç®€åŒ–çš„è®­ç»ƒæ–¹æ³•ï¼ˆåŸºäºPhase 3çš„æµ‹è¯•ç»“æœï¼‰
        from sklearn.preprocessing import StandardScaler
        from sklearn.neural_network import MLPRegressor
        from sklearn.model_selection import train_test_split
        
        # æ•°æ®é¢„å¤„ç†
        scaler = StandardScaler()
        normal_train_scaled = scaler.fit_transform(normal_train)
        
        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        X_train_ae, X_val_ae = train_test_split(
            normal_train_scaled, 
            test_size=0.2, 
            random_state=self.config['random_seed']
        )
        
        # å‡†å¤‡æƒé‡
        weights = self._prepare_weights(wavelengths, self.coating_name)
        
        # æ„å»ºè‡ªç¼–ç å™¨
        encoder = MLPRegressor(
            hidden_layer_sizes=(48, 16, 4),
            activation='relu',
            solver='adam',
            learning_rate_init=1e-3,
            max_iter=200,
            random_state=self.config['random_seed']
        )
        
        decoder = MLPRegressor(
            hidden_layer_sizes=(16, 48, 81),
            activation='relu',
            solver='adam',
            learning_rate_init=1e-3,
            max_iter=200,
            random_state=self.config['random_seed']
        )
        
        # è®­ç»ƒç¼–ç å™¨
        logger.info("è®­ç»ƒç¼–ç å™¨...")
        encoder.fit(X_train_ae, X_train_ae)
        
        # è®­ç»ƒè§£ç å™¨
        logger.info("è®­ç»ƒè§£ç å™¨...")
        latent_train = encoder.predict(X_train_ae)
        decoder.fit(latent_train, X_train_ae)
        
        # è®¡ç®—é‡æ„è¯¯å·®
        def calculate_reconstruction_errors(X):
            errors = []
            for i in range(len(X)):
                latent = encoder.predict(X[i].reshape(1, -1))
                reconstructed = decoder.predict(latent)
                error = np.mean(weights * (X[i] - reconstructed[0]) ** 2)
                errors.append(error)
            return np.array(errors)
        
        train_errors = calculate_reconstruction_errors(X_train_ae)
        val_errors = calculate_reconstruction_errors(X_val_ae)
        
        # è®¡ç®—é˜ˆå€¼
        threshold_percentile = self.config['stability_threshold_percentile']
        stability_threshold = np.percentile(val_errors, threshold_percentile)
        
        autoencoder_result = {
            'encoder_score': float(encoder.score(X_train_ae, X_train_ae)),
            'decoder_score': float(decoder.score(latent_train, X_train_ae)),
            'train_errors': train_errors.tolist(),
            'val_errors': val_errors.tolist(),
            'stability_threshold': float(stability_threshold),
            'threshold_percentile': threshold_percentile,
            'error_stats': {
                'train_mean': float(train_errors.mean()),
                'train_std': float(train_errors.std()),
                'val_mean': float(val_errors.mean()),
                'val_std': float(val_errors.std())
            }
        }
        
        logger.info(f"WeightedAutoencoderè®­ç»ƒå®Œæˆ:")
        logger.info(f"  - ç¼–ç å™¨RÂ²: {autoencoder_result['encoder_score']:.4f}")
        logger.info(f"  - è§£ç å™¨RÂ²: {autoencoder_result['decoder_score']:.4f}")
        logger.info(f"  - ç¨³å®šæ€§é˜ˆå€¼: {stability_threshold:.6f} ({threshold_percentile}%åˆ†ä½æ•°)")
        
        return autoencoder_result, encoder, decoder, scaler, weights
    
    def save_trained_models(self, encoder, decoder, scaler, weights,
                          evaluator_result: dict, autoencoder_result: dict) -> dict:
        """
        ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
        
        Args:
            encoder: è®­ç»ƒå¥½çš„ç¼–ç å™¨
            decoder: è®­ç»ƒå¥½çš„è§£ç å™¨
            scaler: è®­ç»ƒå¥½çš„é¢„å¤„ç†å™¨
            weights: ä½¿ç”¨çš„æƒé‡å‘é‡
            evaluator_result: SimilarityEvaluatorç»“æœ
            autoencoder_result: WeightedAutoencoderç»“æœ
            
        Returns:
            dict: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        # åˆ›å»ºä¿å­˜ç›®å½•
        model_dir = Path("/workspace/code/spectrum_anomaly_detection/models")
        coating_dir = model_dir / self.coating_name / self.version
        coating_dir.mkdir(parents=True, exist_ok=True)
        
        saved_files = {}
        
        # ä¿å­˜ç¼–ç å™¨
        encoder_path = coating_dir / f"encoder_{self.coating_name}_{self.version}.joblib"
        import joblib
        joblib.dump(encoder, encoder_path)
        saved_files['encoder'] = str(encoder_path)
        
        # ä¿å­˜è§£ç å™¨
        decoder_path = coating_dir / f"decoder_{self.coating_name}_{self.version}.joblib"
        joblib.dump(decoder, decoder_path)
        saved_files['decoder'] = str(decoder_path)
        
        # ä¿å­˜é¢„å¤„ç†å™¨
        scaler_path = coating_dir / f"scaler_{self.coating_name}_{self.version}.joblib"
        joblib.dump(scaler, scaler_path)
        saved_files['scaler'] = str(scaler_path)
        
        # ä¿å­˜æƒé‡
        weights_path = coating_dir / f"weights_{self.coating_name}_{self.version}.npy"
        np.save(weights_path, weights)
        saved_files['weights'] = str(weights_path)
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'coating_name': self.coating_name,
            'version': self.version,
            'training_date': datetime.now().isoformat(),
            'config': self.config,
            'similarity_evaluator': {
                'quality_threshold': evaluator_result['quality_threshold'],
                'normal_scores_stats': evaluator_result['normal_scores_stats']
            },
            'weighted_autoencoder': {
                'stability_threshold': autoencoder_result['stability_threshold'],
                'threshold_percentile': autoencoder_result['threshold_percentile'],
                'error_stats': autoencoder_result['error_stats']
            },
            'model_architecture': {
                'input_dim': 81,
                'encoder_dims': [48, 16, 4],
                'decoder_dims': [16, 48, 81],
                'latent_dim': 4
            }
        }
        
        metadata_path = coating_dir / f"metadata_{self.coating_name}_{self.version}.json"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        saved_files['metadata'] = str(metadata_path)
        
        logger.info(f"æ¨¡å‹ä¿å­˜å®Œæˆ: {coating_dir}")
        return saved_files
    
    def generate_training_report(self, evaluator_result: dict, autoencoder_result: dict,
                               saved_files: dict, training_time: float) -> dict:
        """
        ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š
        
        Args:
            evaluator_result: SimilarityEvaluatorç»“æœ
            autoencoder_result: WeightedAutoencoderç»“æœ
            saved_files: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
            training_time: è®­ç»ƒæ—¶é—´
            
        Returns:
            dict: è®­ç»ƒæŠ¥å‘Š
        """
        report = {
            'training_summary': {
                'coating_name': self.coating_name,
                'version': self.version,
                'training_date': datetime.now().isoformat(),
                'training_time_seconds': training_time,
                'status': 'COMPLETED'
            },
            'similarity_evaluator': {
                'quality_threshold': evaluator_result['quality_threshold'],
                'normal_scores_distribution': evaluator_result['normal_scores_stats'],
                'description': 'åŸºäºä¸“å®¶è§„åˆ™çš„è´¨é‡è¯„ä¼°ï¼Œé˜ˆå€¼ç”¨äºåˆ¤æ–­å…‰è°±æ˜¯å¦ç¬¦åˆæ ‡å‡†'
            },
            'weighted_autoencoder': {
                'stability_threshold': autoencoder_result['stability_threshold'],
                'threshold_percentile': autoencoder_result['threshold_percentile'],
                'error_distribution': autoencoder_result['error_stats'],
                'description': 'åŸºäºæœºå™¨å­¦ä¹ çš„ç¨³å®šæ€§è¯„ä¼°ï¼Œé˜ˆå€¼ç”¨äºæ£€æµ‹è¿‡ç¨‹å¼‚å¸¸'
            },
            'model_files': saved_files,
            'next_steps': [
                'Phase 5: æ¨¡å‹è¯„ä¼°å’Œå¯è§†åŒ–ç³»ç»Ÿ',
                'Phase 6: å†³ç­–å¼•æ“å’ŒAPIæ¥å£',
                'éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ'
            ]
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = Path("/workspace/code/spectrum_anomaly_detection/output") / f"training_report_{self.coating_name}_{self.version}.json"
        report_path.parent.mkdir(exist_ok=True)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        return report
    
    def train(self) -> dict:
        """
        æ‰§è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹
        
        Returns:
            dict: è®­ç»ƒç»“æœå’ŒæŠ¥å‘Š
        """
        start_time = datetime.now()
        logger.info(f"å¼€å§‹è®­ç»ƒæµç¨‹: {self.coating_name} {self.version}")
        
        try:
            # æ­¥éª¤1: åŠ è½½æ ‡å‡†æ›²çº¿
            logger.info("æ­¥éª¤1: åŠ è½½æ ‡å‡†æ›²çº¿")
            wavelengths, standard_spectrum = self.load_standard_curve()
            
            # æ­¥éª¤2: ç”Ÿæˆè®­ç»ƒæ•°æ®
            logger.info("æ­¥éª¤2: ç”Ÿæˆè®­ç»ƒæ•°æ®")
            X_train, X_val, y_train, y_val = self.generate_training_data(standard_spectrum, wavelengths)
            
            # æ­¥éª¤3: è®­ç»ƒSimilarityEvaluator
            logger.info("æ­¥éª¤3: é…ç½®SimilarityEvaluator")
            evaluator_result = self.train_similarity_evaluator(X_train, X_val, standard_spectrum, wavelengths)
            
            # æ­¥éª¤4: è®­ç»ƒWeightedAutoencoder
            logger.info("æ­¥éª¤4: è®­ç»ƒWeightedAutoencoder")
            autoencoder_result, encoder, decoder, scaler, weights = self.train_autoencoder(
                X_train, X_val, wavelengths
            )
            
            # æ­¥éª¤5: ä¿å­˜æ¨¡å‹
            logger.info("æ­¥éª¤5: ä¿å­˜æ¨¡å‹")
            saved_files = self.save_trained_models(
                encoder, decoder, scaler, weights, evaluator_result, autoencoder_result
            )
            
            # æ­¥éª¤6: ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š
            logger.info("æ­¥éª¤6: ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š")
            training_time = (datetime.now() - start_time).total_seconds()
            report = self.generate_training_report(
                evaluator_result, autoencoder_result, saved_files, training_time
            )
            
            logger.info(f"è®­ç»ƒæµç¨‹å®Œæˆï¼Œè€—æ—¶ {training_time:.2f} ç§’")
            
            return {
                'status': 'SUCCESS',
                'report': report,
                'saved_files': saved_files,
                'training_time': training_time
            }
            
        except Exception as e:
            logger.error(f"è®­ç»ƒæµç¨‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                'status': 'FAILED',
                'error': str(e),
                'training_time': (datetime.now() - start_time).total_seconds()
            }

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å…‰è°±å¼‚å¸¸æ£€æµ‹æ¨¡å‹è®­ç»ƒè„šæœ¬')
    parser.add_argument('--coating_name', type=str, default='DVP', 
                       help='æ¶‚å±‚åç§° (é»˜è®¤: DVP)')
    parser.add_argument('--version', type=str, default='v1.0',
                       help='æ¨¡å‹ç‰ˆæœ¬å· (é»˜è®¤: v1.0)')
    parser.add_argument('--training_samples', type=int, default=200,
                       help='è®­ç»ƒæ ·æœ¬æ•°é‡ (é»˜è®¤: 200)')
    parser.add_argument('--quality_threshold', type=float, default=85.0,
                       help='Quality Scoreé˜ˆå€¼ (é»˜è®¤: 85.0)')
    parser.add_argument('--stability_threshold_percentile', type=float, default=99.5,
                       help='Stability Scoreé˜ˆå€¼ç™¾åˆ†ä½ (é»˜è®¤: 99.5)')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = SpectrumAnomalyTrainer(args.coating_name, args.version)
    
    # æ›´æ–°é…ç½®
    trainer.config.update({
        'training_samples': args.training_samples,
        'quality_threshold': args.quality_threshold,
        'stability_threshold_percentile': args.stability_threshold_percentile
    })
    
    logger.info(f"è®­ç»ƒé…ç½®: {trainer.config}")
    
    # æ‰§è¡Œè®­ç»ƒ
    result = trainer.train()
    
    # è¾“å‡ºç»“æœ
    if result['status'] == 'SUCCESS':
        print("\n" + "="*60)
        print("ğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆï¼")
        print("="*60)
        print(f"æ¶‚å±‚: {args.coating_name}")
        print(f"ç‰ˆæœ¬: {args.version}")
        print(f"è®­ç»ƒæ—¶é—´: {result['training_time']:.2f} ç§’")
        print(f"Quality Scoreé˜ˆå€¼: {result['report']['similarity_evaluator']['quality_threshold']}%")
        print(f"Stability Scoreé˜ˆå€¼: {result['report']['weighted_autoencoder']['stability_threshold']:.6f}")
        print("\nä¿å­˜çš„æ–‡ä»¶:")
        for file_type, file_path in result['saved_files'].items():
            print(f"  - {file_type}: {os.path.basename(file_path)}")
    else:
        print("\n" + "="*60)
        print("âŒ è®­ç»ƒå¤±è´¥ï¼")
        print("="*60)
        print(f"é”™è¯¯: {result['error']}")
    
    return result['status'] == 'SUCCESS'

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)