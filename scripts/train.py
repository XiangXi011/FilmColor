#!/usr/bin/env python3
"""
æ¨¡å‹è®­ç»ƒè„šæœ¬ - train.py
å®Œæ•´çš„DVPæ¶‚å±‚ç±»å‹å…‰è°±å¼‚å¸¸æ£€æµ‹æ¨¡å‹è®­ç»ƒæµç¨‹

åŠŸèƒ½ï¼š
- æ”¯æŒå‘½ä»¤è¡Œå‚æ•°æŒ‡å®šcoating_name
- æ•´åˆSimilarityEvaluatorå’ŒWeightedAutoencoder
- å®ç°æ¨¡å‹ä¿å­˜åŠŸèƒ½(.h5, .pkl, .json)
- é˜ˆå€¼è®¡ç®—(99.5%åˆ†ä½æ•°)
- å¤šäº§å“ç±»å‹è®­ç»ƒæ”¯æŒ
- å®Œæ•´çš„è®­ç»ƒæ—¥å¿—å’ŒæŠ¥å‘Š
"""

import os
import sys
import argparse
import json
import numpy as np
import pandas as pd
from pathlib import Path
import logging
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/workspace/code/spectrum_anomaly_detection')

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from algorithms.similarity_evaluator import SimilarityEvaluator
# from models.weighted_autoencoder import WeightedAutoencoder  # æš‚æ—¶æ³¨é‡Šæ‰TensorFlowç‰ˆæœ¬

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class SpectrumAnomalyTrainer:
    """
    å…‰è°±å¼‚å¸¸æ£€æµ‹æ¨¡å‹è®­ç»ƒå™¨
    
    æ•´åˆQuality Scoreå’ŒStability Scoreçš„è®­ç»ƒæµç¨‹
    """
    
    def __init__(self, coating_name: str = "DVP", version: str = "v1.0"):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            coating_name: æ¶‚å±‚åç§°
            version: æ¨¡å‹ç‰ˆæœ¬å·
        """
        self.coating_name = coating_name
        self.version = version
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.similarity_evaluator = SimilarityEvaluator(coating_name)
        # self.autoencoder = WeightedAutoencoder(input_dim=81, coating_name=coating_name)  # æš‚æ—¶æ³¨é‡Šæ‰TensorFlowç‰ˆæœ¬
        
        # è®­ç»ƒé…ç½®
        self.config = {
            'training_samples': 200,
            'validation_split': 0.2,
            'noise_levels': [0.05, 0.1, 0.15, 0.2],
            'quality_threshold': 92.0,  # æé«˜é»˜è®¤Quality Scoreé˜ˆå€¼ï¼Œé™ä½è¯¯æŠ¥
            'stability_threshold_percentile': 99.9,  # æé«˜é»˜è®¤åˆ†ä½é˜ˆå€¼ï¼Œé™ä½è¯¯æŠ¥
            'stability_threshold_method': 'percentile',  # 'percentile' | 'mad'
            'stability_mad_k': 3.5,  # å½“ä½¿ç”¨MADæ³•æ—¶çš„å€æ•°
            # æƒé‡ç›¸å…³é…ç½®ï¼ˆå¯é€šè¿‡å‘½ä»¤è¡Œè¦†ç›–ï¼‰
            'weights_base_range': [400.0, 680.0],  # åŸºç¡€åŠ æƒåŒºé—´
            'weights_base_value': 3.0,            # åŸºç¡€æƒé‡å€¼
            'weights_peak_range': [400.0, 560.0], # é‡ç‚¹å¢å¼ºåŒºé—´ï¼ˆå¾®è°ƒä¸Šé™ï¼‰
            'weights_peak_multiplier': 1.5,       # é‡ç‚¹åŒºé—´å€ç‡
            'weights_mode': 'static',             # static | variance | variance_smooth | hybrid
            'weights_mix_alpha': 0.7,             # hybrid: w = alpha*static + (1-alpha)*variance_smooth
            'weights_smooth_pctl_lo': 5.0,        # variance_smooth: winsorizeä¸‹é™ç™¾åˆ†ä½
            'weights_smooth_pctl_hi': 95.0,       # variance_smooth: winsorizeä¸Šé™ç™¾åˆ†ä½
            'weights_smooth_window': 5,           # variance_smooth: æ»‘åŠ¨çª—å£å¤§å°(å¥‡æ•°)
            'random_seed': 42,
            # AEé»˜è®¤å›ºåŒ–é…ç½®ï¼ˆæ— éœ€æ¯æ¬¡ä¼ å‚ï¼‰
            'ae_encoder_layers': [64, 32, 16, 8],
            'ae_decoder_layers': [16, 32, 64, 81],
            'ae_alpha': 2e-4,
            'ae_early_stopping': True,
            'ae_n_iter_no_change': 12,
            'ae_validation_fraction': 0.1
        }
        
        logger.info(f"SpectrumAnomalyTrainer åˆå§‹åŒ–å®Œæˆ [{coating_name} {version}]")
    
    def _prepare_weights(self, wavelengths: np.ndarray, coating_name: str) -> np.ndarray:
        """
        å‡†å¤‡æƒé‡å‘é‡ï¼ˆä¸WeightedAutoencoderä¸­çš„æ–¹æ³•ç›¸åŒï¼‰
        
        Args:
            wavelengths: æ³¢é•¿æ•°ç»„
            coating_name: æ¶‚å±‚åç§°
            
        Returns:
            np.ndarray: æƒé‡å‘é‡
        """
        weights = np.ones_like(wavelengths, dtype=np.float64)
        
        # åŸºç¡€æƒé‡åŒºé—´ä¸æ•°å€¼
        base_lo, base_hi = self.config.get('weights_base_range', [400.0, 680.0])
        base_val = float(self.config.get('weights_base_value', 3.0))
        mask = (wavelengths >= base_lo) & (wavelengths <= base_hi)
        weights[mask] = base_val
        
        # é‡ç‚¹å¢å¼ºåŒºé—´
        if coating_name == "DVP":
            peak_lo, peak_hi = self.config.get('weights_peak_range', [400.0, 560.0])
            peak_mul = float(self.config.get('weights_peak_multiplier', 1.5))
            peak_mask = (wavelengths >= peak_lo) & (wavelengths <= peak_hi)
            weights[peak_mask] *= peak_mul
        
        # å½’ä¸€åŒ–æƒé‡
        weights = weights / np.mean(weights)
        
        return weights
    def _prepare_weights(self, wavelengths: np.ndarray, coating_name: str) -> np.ndarray:
        """
        å‡†å¤‡æƒé‡å‘é‡ï¼ˆä¸WeightedAutoencoderä¸­çš„æ–¹æ³•ç›¸åŒï¼‰
        
        Args:
            wavelengths: æ³¢é•¿æ•°ç»„
            coating_name: æ¶‚å±‚åç§°
            
        Returns:
            np.ndarray: æƒé‡å‘é‡
        """
        # æŒ‰é…ç½®é€‰æ‹©æƒé‡æ¨¡å¼
        mode = (self.config.get('weights_mode') or 'static').lower()
        if mode in ('variance', 'variance_smooth', 'hybrid'):
            # åŸºäºè®­ç»ƒæ•°æ®NPZçŸ©é˜µçš„æ³¢é•¿æ–¹å·®æ„é€ æƒé‡ï¼šw âˆ 1/(std+eps)
            try:
                project_root = Path(__file__).parent.parent
                default_path = str(project_root / "data" / "dvp_processed_data.npz")
                data_path = getattr(self, 'data_npz_path', None) or default_path
                data = np.load(data_path)
                spectra = data['dvp_values']
                if spectra.ndim == 2 and spectra.shape[1] == len(wavelengths):
                    std = np.std(spectra, axis=0)
                    # winsorizeæˆªæ–­
                    if mode in ('variance_smooth', 'hybrid'):
                        lo = float(self.config.get('weights_smooth_pctl_lo', 5.0))
                        hi = float(self.config.get('weights_smooth_pctl_hi', 95.0))
                        lo_v = np.percentile(std, lo)
                        hi_v = np.percentile(std, hi)
                        std = np.clip(std, lo_v, hi_v)
                    eps = 1e-8
                    inv_std = 1.0 / (std + eps)
                    weights_var = inv_std.astype(np.float64)
                    # å¹³æ»‘
                    if mode in ('variance_smooth', 'hybrid'):
                        win = int(self.config.get('weights_smooth_window', 5))
                        win = max(1, win)
                        if win > 1:
                            # ç®€å•ç§»åŠ¨å¹³å‡
                            kernel = np.ones(win, dtype=np.float64) / win
                            pad = win // 2
                            padded = np.pad(weights_var, (pad, pad), mode='edge')
                            weights_var = np.convolve(padded, kernel, mode='valid')
                    # å½’ä¸€åŒ–
                    weights_var = weights_var / np.mean(weights_var)
                    if mode == 'hybrid':
                        # è®¡ç®—staticæƒé‡
                        w_static = np.ones_like(wavelengths, dtype=np.float64)
                        base_lo, base_hi = self.config.get('weights_base_range', [400.0, 680.0])
                        base_val = float(self.config.get('weights_base_value', 3.0))
                        mask = (wavelengths >= base_lo) & (wavelengths <= base_hi)
                        w_static[mask] = base_val
                        if coating_name == "DVP":
                            peak_lo, peak_hi = self.config.get('weights_peak_range', [400.0, 560.0])
                            peak_mul = float(self.config.get('weights_peak_multiplier', 1.5))
                            peak_mask = (wavelengths >= peak_lo) & (wavelengths <= peak_hi)
                            w_static[peak_mask] *= peak_mul
                        w_static = w_static / np.mean(w_static)
                        alpha = float(self.config.get('weights_mix_alpha', 0.7))
                        weights = alpha * w_static + (1.0 - alpha) * weights_var
                        weights = weights / np.mean(weights)
                        return weights
                    else:
                        return weights_var
            except Exception:
                pass  # å›é€€åˆ°static

        weights = np.ones_like(wavelengths, dtype=np.float64)
        
        # åŸºç¡€æƒé‡: 400-680nmèŒƒå›´æƒé‡ä¸º3
        mask = (wavelengths >= 400) & (wavelengths <= 680)
        weights[mask] = 3.0
        
        # æ ¹æ®æ¶‚å±‚ç±»å‹è°ƒæ•´æƒé‡
        if coating_name == "DVP":
            # DVPæ¶‚å±‚: å¢å¼º400-560nmæ³¢æ®µçš„æƒé‡
            peak_mask = (wavelengths >= 400) & (wavelengths <= 560)
            weights[peak_mask] *= 1.5
        
        # å½’ä¸€åŒ–æƒé‡
        weights = weights / np.mean(weights)
        
        return weights
    def load_standard_curve(self) -> tuple:
        """
        åŠ è½½æ ‡å‡†æ›²çº¿æ•°æ®
        
        Returns:
            tuple: (æ³¢é•¿æ•°ç»„, æ ‡å‡†å…‰è°±æ•°ç»„)
        """
        # ä½¿ç”¨é¡¹ç›®ç›¸å¯¹è·¯å¾„ï¼Œæˆ–å‘½ä»¤è¡Œä¼ å…¥çš„è‡ªå®šä¹‰NPZè·¯å¾„
        project_root = Path(__file__).parent.parent
        default_path = str(project_root / "data" / "dvp_processed_data.npz")
        data_path = getattr(self, 'data_npz_path', None) or default_path
        
        if not os.path.exists(data_path):
            raise FileNotFoundError(f"æ ‡å‡†æ›²çº¿æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        
        data = np.load(data_path)
        # åŸºæœ¬æ•°æ®æ ¡éªŒ
        if 'wavelengths' not in data or 'dvp_values' not in data:
            raise ValueError("NPZç¼ºå°‘å¿…é¡»å­—æ®µ: wavelengths æˆ– dvp_values")
        wavelengths = data['wavelengths']
        if np.isnan(wavelengths).any():
            raise ValueError("wavelengths å«æœ‰ NaN")
        
        # æ ¹æ®æ¶‚å±‚åç§°é€‰æ‹©å¯¹åº”æ•°æ®çŸ©é˜µï¼ˆæ ·æœ¬æ•° Ã— æ³¢é•¿æ•°ï¼‰
        if self.coating_name == "DVP":
            spectra = data['dvp_values']
        else:
            spectra = data['dvp_values']
            logger.warning(f"æ¶‚å±‚ {self.coating_name} ä½¿ç”¨DVPæ•°æ®ä½œä¸ºç¤ºä¾‹")

        # å½“è¾“å…¥ä¸ºæ ·æœ¬çŸ©é˜µæ—¶ï¼Œéœ€æ±‡èšä¸ºä¸€æ¡æ ‡å‡†æ›²çº¿ï¼ˆé•¿åº¦==æ³¢é•¿æ•°ï¼‰
        if spectra.ndim == 2:
            if spectra.shape[1] != len(wavelengths):
                raise ValueError(f"å…‰è°±çŸ©é˜µåˆ—æ•°({spectra.shape[1]})ä¸æ³¢é•¿æ•°({len(wavelengths)})ä¸ä¸€è‡´")
            if np.isnan(spectra).any():
                raise ValueError("å…‰è°±çŸ©é˜µå«æœ‰ NaN")
            # é‡‡ç”¨ä¸­ä½æ•°æ›´ç¨³å¥ï¼ˆé˜²å¼‚å¸¸å€¼ï¼‰ï¼Œä¹Ÿå¯åˆ‡æ¢ä¸ºå‡å€¼
            agg = getattr(self, 'std_curve_agg', 'median')
            if agg == 'mean':
                standard_spectrum = np.mean(spectra, axis=0)
            else:
                standard_spectrum = np.median(spectra, axis=0)
            logger.info(f"ä» {spectra.shape[0]} æ¡æ ·æœ¬æ±‡èšå¾—åˆ°æ ‡å‡†æ›²çº¿ï¼ˆä½¿ç”¨ {agg}ï¼‰")
        else:
            standard_spectrum = spectra
        
        logger.info(f"æ ‡å‡†æ›²çº¿åŠ è½½å®Œæˆ: {len(wavelengths)}ä¸ªæ³¢é•¿ç‚¹ | æ¥æº: {data_path}")
        return wavelengths, standard_spectrum
    
    def generate_training_data(self, standard_spectrum: np.ndarray, 
                             wavelengths: np.ndarray) -> tuple:
        """
        ç”Ÿæˆè®­ç»ƒæ•°æ®
        
        Args:
            standard_spectrum: æ ‡å‡†å…‰è°±
            wavelengths: æ³¢é•¿æ•°ç»„
            
        Returns:
            tuple: (è®­ç»ƒå…‰è°±, éªŒè¯å…‰è°±, æ ‡ç­¾)
        """
        np.random.seed(self.config['random_seed'])
        
        # ç”Ÿæˆæ­£å¸¸æ ·æœ¬ï¼ˆä½å™ªå£°ï¼‰
        normal_spectra = []
        normal_labels = []
        
        n_samples = self.config['training_samples']
        noise_levels = self.config['noise_levels']
        
        for i in range(n_samples):
            # éšæœºé€‰æ‹©å™ªå£°æ°´å¹³
            noise_level = np.random.choice(noise_levels)
            
            # ç”Ÿæˆå™ªå£°
            noise = np.random.normal(0, noise_level, len(standard_spectrum))
            
            # ç”Ÿæˆå…‰è°±
            spectrum = standard_spectrum + noise
            
            normal_spectra.append(spectrum)
            normal_labels.append(1)  # 1è¡¨ç¤ºæ­£å¸¸
        
        # ç”Ÿæˆä¸€äº›å¼‚å¸¸æ ·æœ¬ï¼ˆé«˜å™ªå£°ï¼‰ç”¨äºéªŒè¯
        anomaly_spectra = []
        anomaly_labels = []
        
        for i in range(50):  # ç”Ÿæˆ50ä¸ªå¼‚å¸¸æ ·æœ¬
            # é«˜å™ªå£°æ°´å¹³
            noise_level = np.random.uniform(0.5, 1.5)
            
            # ç”Ÿæˆå™ªå£°
            noise = np.random.normal(0, noise_level, len(standard_spectrum))
            
            # ç”Ÿæˆå…‰è°±
            spectrum = standard_spectrum + noise
            
            anomaly_spectra.append(spectrum)
            anomaly_labels.append(0)  # 0è¡¨ç¤ºå¼‚å¸¸
        
        # åˆå¹¶æ•°æ®
        all_spectra = np.array(normal_spectra + anomaly_spectra)
        all_labels = np.array(normal_labels + anomaly_labels)
        
        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        from sklearn.model_selection import train_test_split
        
        X_train, X_val, y_train, y_val = train_test_split(
            all_spectra, all_labels, 
            test_size=self.config['validation_split'], 
            random_state=self.config['random_seed'],
            stratify=all_labels
        )
        
        logger.info(f"è®­ç»ƒæ•°æ®ç”Ÿæˆå®Œæˆ:")
        logger.info(f"  - è®­ç»ƒé›†: {len(X_train)}ä¸ªæ ·æœ¬ (æ­£å¸¸{y_train.sum()}, å¼‚å¸¸{len(y_train)-y_train.sum()})")
        logger.info(f"  - éªŒè¯é›†: {len(X_val)}ä¸ªæ ·æœ¬ (æ­£å¸¸{y_val.sum()}, å¼‚å¸¸{len(y_val)-y_val.sum()})")
        
        return X_train, X_val, y_train, y_val
    
    def train_similarity_evaluator(self, X_train: np.ndarray, X_val: np.ndarray,
                                 standard_spectrum: np.ndarray, wavelengths: np.ndarray) -> dict:
        """
        è®­ç»ƒSimilarityEvaluatorï¼ˆå®é™…ä¸Šä¸éœ€è¦è®­ç»ƒï¼Œåªæ˜¯é…ç½®ï¼‰
        
        Args:
            X_train: è®­ç»ƒå…‰è°±
            X_val: éªŒè¯å…‰è°±
            standard_spectrum: æ ‡å‡†å…‰è°±
            wavelengths: æ³¢é•¿æ•°ç»„
            
        Returns:
            dict: è¯„ä¼°ç»“æœ
        """
        logger.info("é…ç½®SimilarityEvaluator...")
        
        # å¯¹è®­ç»ƒé›†å’ŒéªŒè¯é›†è¿›è¡ŒQuality Scoreè¯„ä¼°
        train_results = self.similarity_evaluator.batch_evaluate(
            X_train, standard_spectrum, wavelengths, self.coating_name
        )
        
        val_results = self.similarity_evaluator.batch_evaluate(
            X_val, standard_spectrum, wavelengths, self.coating_name
        )
        
        # è®¡ç®—Quality Scoreé˜ˆå€¼
        quality_threshold = self.config['quality_threshold']
        
        # ç»Ÿè®¡æ­£å¸¸æ ·æœ¬çš„Quality Scoreåˆ†å¸ƒ
        normal_train_scores = train_results[train_results['spectrum_id'] < len(X_train)//2]['similarity_score_percent']
        normal_val_scores = val_results[val_results['spectrum_id'] < len(X_val)//2]['similarity_score_percent']
        
        all_normal_scores = np.concatenate([normal_train_scores, normal_val_scores])
        
        evaluator_result = {
            'training_scores': train_results.to_dict('records'),
            'validation_scores': val_results.to_dict('records'),
            'quality_threshold': quality_threshold,
            'normal_scores_stats': {
                'mean': float(all_normal_scores.mean()),
                'std': float(all_normal_scores.std()),
                'min': float(all_normal_scores.min()),
                'max': float(all_normal_scores.max()),
                'percentile_5': float(np.percentile(all_normal_scores, 5)),
                'percentile_95': float(np.percentile(all_normal_scores, 95))
            }
        }
        
        logger.info(f"SimilarityEvaluatoré…ç½®å®Œæˆ:")
        logger.info(f"  - Quality Scoreé˜ˆå€¼: {quality_threshold}%")
        logger.info(f"  - æ­£å¸¸æ ·æœ¬å¹³å‡åˆ†æ•°: {evaluator_result['normal_scores_stats']['mean']:.2f}%")
        
        return evaluator_result
    
    def train_autoencoder(self, X_train: np.ndarray, X_val: np.ndarray, 
                         y_train: np.ndarray, wavelengths: np.ndarray) -> dict:
        """
        è®­ç»ƒWeightedAutoencoder
        
        Args:
            X_train: è®­ç»ƒå…‰è°±
            X_val: éªŒè¯å…‰è°±
            wavelengths: æ³¢é•¿æ•°ç»„
            
        Returns:
            dict: è®­ç»ƒç»“æœ
        """
        logger.info("å¼€å§‹è®­ç»ƒWeightedAutoencoder...")
        
        # åªä½¿ç”¨æ­£å¸¸æ ·æœ¬è®­ç»ƒè‡ªç¼–ç å™¨
        normal_train = X_train[y_train == 1]
        
        # ä½¿ç”¨ç®€åŒ–çš„è®­ç»ƒæ–¹æ³•ï¼ˆåŸºäºPhase 3çš„æµ‹è¯•ç»“æœï¼‰
        from sklearn.preprocessing import StandardScaler
        from sklearn.neural_network import MLPRegressor
        from sklearn.model_selection import train_test_split
        
        # æ•°æ®é¢„å¤„ç†
        scaler = StandardScaler()
        normal_train_scaled = scaler.fit_transform(normal_train)
        
        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        X_train_ae, X_val_ae = train_test_split(
            normal_train_scaled, 
            test_size=0.2, 
            random_state=self.config['random_seed']
        )
        
        # å‡†å¤‡æƒé‡
        weights = self._prepare_weights(wavelengths, self.coating_name)
        
        # æ„å»ºè‡ªç¼–ç å™¨
        # å¯é…ç½®çš„ç½‘ç»œç»“æ„ä¸æ­£åˆ™åŒ–
        enc_layers = tuple(self.config.get('ae_encoder_layers', [64, 32, 16, 8]))
        dec_layers = tuple(self.config.get('ae_decoder_layers', [16, 32, 64, 81]))
        alpha = float(self.config.get('ae_alpha', 1e-4))
        early_stopping = bool(self.config.get('ae_early_stopping', True))
        n_iter_no_change = int(self.config.get('ae_n_iter_no_change', 10))
        val_fraction = float(self.config.get('ae_validation_fraction', 0.1))

        encoder = MLPRegressor(
            hidden_layer_sizes=enc_layers,
            activation='relu',
            solver='adam',
            learning_rate_init=1e-3,
            max_iter=400,
            alpha=alpha,
            early_stopping=early_stopping,
            n_iter_no_change=n_iter_no_change,
            validation_fraction=val_fraction,
            random_state=self.config['random_seed']
        )
        
        decoder = MLPRegressor(
            hidden_layer_sizes=dec_layers,
            activation='relu',
            solver='adam',
            learning_rate_init=1e-3,
            max_iter=400,
            alpha=alpha,
            early_stopping=early_stopping,
            n_iter_no_change=n_iter_no_change,
            validation_fraction=val_fraction,
            random_state=self.config['random_seed']
        )
        
        # è®­ç»ƒç¼–ç å™¨
        logger.info("è®­ç»ƒç¼–ç å™¨...")
        encoder.fit(X_train_ae, X_train_ae)
        
        # è®­ç»ƒè§£ç å™¨
        logger.info("è®­ç»ƒè§£ç å™¨...")
        latent_train = encoder.predict(X_train_ae)
        decoder.fit(latent_train, X_train_ae)
        
        # è®¡ç®—é‡æ„è¯¯å·®
        def calculate_reconstruction_errors(X):
            errors = []
            for i in range(len(X)):
                latent = encoder.predict(X[i].reshape(1, -1))
                reconstructed = decoder.predict(latent)
                error = np.mean(weights * (X[i] - reconstructed[0]) ** 2)
                errors.append(error)
            return np.array(errors)
        
        train_errors = calculate_reconstruction_errors(X_train_ae)
        val_errors = calculate_reconstruction_errors(X_val_ae)
        
        # è®¡ç®—é˜ˆå€¼ï¼ˆæ”¯æŒåˆ†ä½æ•°æˆ–MADç¨³å¥æ–¹æ³•ï¼‰
        method = self.config.get('stability_threshold_method', 'percentile')
        if method == 'mad':
            median = np.median(val_errors)
            mad = np.median(np.abs(val_errors - median)) + 1e-12
            k = float(self.config.get('stability_mad_k', 3.5))
            stability_threshold = float(median + k * 1.4826 * mad)
            threshold_percentile = None
        else:
            threshold_percentile = float(self.config['stability_threshold_percentile'])
            stability_threshold = float(np.percentile(val_errors, threshold_percentile))
        
        autoencoder_result = {
            'encoder_score': float(encoder.score(X_train_ae, X_train_ae)),
            'decoder_score': float(decoder.score(latent_train, X_train_ae)),
            'train_errors': train_errors.tolist(),
            'val_errors': val_errors.tolist(),
            'stability_threshold': float(stability_threshold),
            'threshold_method': method,
            'threshold_percentile': threshold_percentile,
            'error_stats': {
                'train_mean': float(train_errors.mean()),
                'train_std': float(train_errors.std()),
                'val_mean': float(val_errors.mean()),
                'val_std': float(val_errors.std())
            }
        }
        
        logger.info(f"WeightedAutoencoderè®­ç»ƒå®Œæˆ:")
        logger.info(f"  - ç¼–ç å™¨RÂ²: {autoencoder_result['encoder_score']:.4f}")
        logger.info(f"  - è§£ç å™¨RÂ²: {autoencoder_result['decoder_score']:.4f}")
        if method == 'mad':
            logger.info(f"  - ç¨³å®šæ€§é˜ˆå€¼: {stability_threshold:.6f} (MADæ³• k={self.config.get('stability_mad_k', 3.5)})")
        else:
            logger.info(f"  - ç¨³å®šæ€§é˜ˆå€¼: {stability_threshold:.6f} ({threshold_percentile}%åˆ†ä½æ•°)")
        
        return autoencoder_result, encoder, decoder, scaler, weights
    
    def save_trained_models(self, encoder, decoder, scaler, weights,
                          evaluator_result: dict, autoencoder_result: dict) -> dict:
        """
        ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
        
        Args:
            encoder: è®­ç»ƒå¥½çš„ç¼–ç å™¨
            decoder: è®­ç»ƒå¥½çš„è§£ç å™¨
            scaler: è®­ç»ƒå¥½çš„é¢„å¤„ç†å™¨
            weights: ä½¿ç”¨çš„æƒé‡å‘é‡
            evaluator_result: SimilarityEvaluatorç»“æœ
            autoencoder_result: WeightedAutoencoderç»“æœ
            
        Returns:
            dict: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        # åˆ›å»ºä¿å­˜ç›®å½•
        model_dir = Path("/workspace/code/spectrum_anomaly_detection/models")
        coating_dir = model_dir / self.coating_name / self.version
        coating_dir.mkdir(parents=True, exist_ok=True)
        
        saved_files = {}
        
        # ä¿å­˜ç¼–ç å™¨
        encoder_path = coating_dir / f"encoder_{self.coating_name}_{self.version}.joblib"
        import joblib
        joblib.dump(encoder, encoder_path)
        saved_files['encoder'] = str(encoder_path)
        
        # ä¿å­˜è§£ç å™¨
        decoder_path = coating_dir / f"decoder_{self.coating_name}_{self.version}.joblib"
        joblib.dump(decoder, decoder_path)
        saved_files['decoder'] = str(decoder_path)
        
        # ä¿å­˜é¢„å¤„ç†å™¨
        scaler_path = coating_dir / f"scaler_{self.coating_name}_{self.version}.joblib"
        joblib.dump(scaler, scaler_path)
        saved_files['scaler'] = str(scaler_path)
        
        # ä¿å­˜æƒé‡
        weights_path = coating_dir / f"weights_{self.coating_name}_{self.version}.npy"
        np.save(weights_path, weights)
        saved_files['weights'] = str(weights_path)
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'coating_name': self.coating_name,
            'version': self.version,
            'training_date': datetime.now().isoformat(),
            'config': self.config,
            'similarity_evaluator': {
                'quality_threshold': evaluator_result['quality_threshold'],
                'normal_scores_stats': evaluator_result['normal_scores_stats']
            },
            'weighted_autoencoder': {
                'stability_threshold': autoencoder_result['stability_threshold'],
                'threshold_method': autoencoder_result.get('threshold_method', 'percentile'),
                'threshold_percentile': autoencoder_result.get('threshold_percentile'),
                'error_stats': autoencoder_result['error_stats']
            },
            'combine_strategy': 'two_stage',
            'model_architecture': {
                'input_dim': 81,
                'encoder_dims': [48, 16, 4],
                'decoder_dims': [16, 48, 81],
                'latent_dim': 4
            }
        }
        # è®°å½•æƒé‡æ¨¡å¼ä¸å…³é”®å‚æ•°
        metadata['weighted_autoencoder']['weights_mode'] = self.config.get('weights_mode', 'static')
        metadata['weighted_autoencoder']['weights_params'] = {
            'mix_alpha': self.config.get('weights_mix_alpha', 0.7),
            'smooth_pctl_lo': self.config.get('weights_smooth_pctl_lo', 5.0),
            'smooth_pctl_hi': self.config.get('weights_smooth_pctl_hi', 95.0),
            'smooth_window': self.config.get('weights_smooth_window', 5),
            'base_range': self.config.get('weights_base_range', [400.0, 680.0]),
            'base_value': self.config.get('weights_base_value', 3.0),
            'peak_range': self.config.get('weights_peak_range', [400.0, 560.0]),
            'peak_multiplier': self.config.get('weights_peak_multiplier', 1.5),
        }
        
        metadata_path = coating_dir / f"metadata_{self.coating_name}_{self.version}.json"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        saved_files['metadata'] = str(metadata_path)
        
        logger.info(f"æ¨¡å‹ä¿å­˜å®Œæˆ: {coating_dir}")
        return saved_files
    
    def generate_training_report(self, evaluator_result: dict, autoencoder_result: dict,
                               saved_files: dict, training_time: float) -> dict:
        """
        ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š
        
        Args:
            evaluator_result: SimilarityEvaluatorç»“æœ
            autoencoder_result: WeightedAutoencoderç»“æœ
            saved_files: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
            training_time: è®­ç»ƒæ—¶é—´
            
        Returns:
            dict: è®­ç»ƒæŠ¥å‘Š
        """
        report = {
            'training_summary': {
                'coating_name': self.coating_name,
                'version': self.version,
                'training_date': datetime.now().isoformat(),
                'training_time_seconds': training_time,
                'status': 'COMPLETED'
            },
            'similarity_evaluator': {
                'quality_threshold': evaluator_result['quality_threshold'],
                'normal_scores_distribution': evaluator_result['normal_scores_stats'],
                'description': 'åŸºäºä¸“å®¶è§„åˆ™çš„è´¨é‡è¯„ä¼°ï¼Œé˜ˆå€¼ç”¨äºåˆ¤æ–­å…‰è°±æ˜¯å¦ç¬¦åˆæ ‡å‡†'
            },
            'weighted_autoencoder': {
                'stability_threshold': autoencoder_result['stability_threshold'],
                'threshold_percentile': autoencoder_result['threshold_percentile'],
                'error_distribution': autoencoder_result['error_stats'],
                'description': 'åŸºäºæœºå™¨å­¦ä¹ çš„ç¨³å®šæ€§è¯„ä¼°ï¼Œé˜ˆå€¼ç”¨äºæ£€æµ‹è¿‡ç¨‹å¼‚å¸¸'
            },
            'model_files': saved_files,
            'next_steps': [
                'Phase 5: æ¨¡å‹è¯„ä¼°å’Œå¯è§†åŒ–ç³»ç»Ÿ',
                'Phase 6: å†³ç­–å¼•æ“å’ŒAPIæ¥å£',
                'éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ'
            ]
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = Path("/workspace/code/spectrum_anomaly_detection/output") / f"training_report_{self.coating_name}_{self.version}.json"
        report_path.parent.mkdir(exist_ok=True)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        return report
    
    def train(self) -> dict:
        """
        æ‰§è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹
        
        Returns:
            dict: è®­ç»ƒç»“æœå’ŒæŠ¥å‘Š
        """
        start_time = datetime.now()
        logger.info(f"å¼€å§‹è®­ç»ƒæµç¨‹: {self.coating_name} {self.version}")
        
        try:
            # æ­¥éª¤1: åŠ è½½æ ‡å‡†æ›²çº¿
            logger.info("æ­¥éª¤1: åŠ è½½æ ‡å‡†æ›²çº¿")
            wavelengths, standard_spectrum = self.load_standard_curve()
            
            # æ­¥éª¤2: ç”Ÿæˆè®­ç»ƒæ•°æ®
            logger.info("æ­¥éª¤2: ç”Ÿæˆè®­ç»ƒæ•°æ®")
            X_train, X_val, y_train, y_val = self.generate_training_data(standard_spectrum, wavelengths)
            
            # æ­¥éª¤3: è®­ç»ƒSimilarityEvaluator
            logger.info("æ­¥éª¤3: é…ç½®SimilarityEvaluator")
            evaluator_result = self.train_similarity_evaluator(X_train, X_val, standard_spectrum, wavelengths)
            
            # æ­¥éª¤4: è®­ç»ƒWeightedAutoencoder
            logger.info("æ­¥éª¤4: è®­ç»ƒWeightedAutoencoder")
            autoencoder_result, encoder, decoder, scaler, weights = self.train_autoencoder(
                X_train, X_val, y_train, wavelengths
            )
            
            # æ­¥éª¤5: ä¿å­˜æ¨¡å‹
            logger.info("æ­¥éª¤5: ä¿å­˜æ¨¡å‹")
            saved_files = self.save_trained_models(
                encoder, decoder, scaler, weights, evaluator_result, autoencoder_result
            )
            
            # æ­¥éª¤6: ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š
            logger.info("æ­¥éª¤6: ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š")
            training_time = (datetime.now() - start_time).total_seconds()
            report = self.generate_training_report(
                evaluator_result, autoencoder_result, saved_files, training_time
            )
            
            logger.info(f"è®­ç»ƒæµç¨‹å®Œæˆï¼Œè€—æ—¶ {training_time:.2f} ç§’")
            
            return {
                'status': 'SUCCESS',
                'report': report,
                'saved_files': saved_files,
                'training_time': training_time
            }
            
        except Exception as e:
            logger.error(f"è®­ç»ƒæµç¨‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                'status': 'FAILED',
                'error': str(e),
                'training_time': (datetime.now() - start_time).total_seconds()
            }

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å…‰è°±å¼‚å¸¸æ£€æµ‹æ¨¡å‹è®­ç»ƒè„šæœ¬')
    parser.add_argument('--coating_name', type=str, default='DVP', 
                       help='æ¶‚å±‚åç§° (é»˜è®¤: DVP)')
    parser.add_argument('--version', type=str, default='v1.0',
                       help='æ¨¡å‹ç‰ˆæœ¬å· (é»˜è®¤: v1.0)')
    parser.add_argument('--training_samples', type=int, default=200,
                       help='è®­ç»ƒæ ·æœ¬æ•°é‡ (é»˜è®¤: 200)')
    parser.add_argument('--quality_threshold', type=float, default=92.0,
                       help='Quality Scoreé˜ˆå€¼ (é»˜è®¤: 92.0)')
    parser.add_argument('--stability_threshold_percentile', type=float, default=99.9,
                       help='Stability Scoreé˜ˆå€¼ç™¾åˆ†ä½ (é»˜è®¤: 99.9)')
    parser.add_argument('--stability_threshold_method', type=str, default='percentile',
                       choices=['percentile', 'mad'],
                       help='ç¨³å®šæ€§é˜ˆå€¼æ–¹æ³•ï¼špercentile æˆ– mad (é»˜è®¤: percentile)')
    parser.add_argument('--stability_mad_k', type=float, default=3.5,
                       help='MADæ³•çš„kå€æ•° (é»˜è®¤: 3.5)')
    parser.add_argument('--std_curve_agg', type=str, choices=['median','mean'], default='median',
                       help='æ ‡å‡†æ›²çº¿æ±‡èšæ–¹æ³•ï¼ˆmedian/meanï¼‰ï¼Œé»˜è®¤median')
    parser.add_argument('--data-npz', type=str, default=None,
                       help='è‡ªå®šä¹‰è®­ç»ƒæ•°æ®NPZè·¯å¾„ï¼ˆåŒ…å« wavelengths, dvp_valuesï¼‰')
    parser.add_argument('--weights-mode', type=str, default='static', choices=['static','variance','variance_smooth','hybrid'],
                       help='æƒé‡æ¨¡å¼ï¼šstatic å›ºå®šï¼›variance æ–¹å·®å€’æ•°ï¼›variance_smooth æ–¹å·®å€’æ•°+æˆªæ–­+å¹³æ»‘ï¼›hybrid æ··åˆ')
    parser.add_argument('--weights-mix-alpha', type=float, default=0.7,
                       help='hybridæ··åˆæƒé‡Î±ï¼Œè¶Šå¤§è¶Šåå‘static (é»˜è®¤0.7)')
    parser.add_argument('--weights-smooth-pctl-lo', type=float, default=5.0,
                       help='variance_smooth æˆªæ–­ä¸‹ç™¾åˆ†ä½ (é»˜è®¤5)')
    parser.add_argument('--weights-smooth-pctl-hi', type=float, default=95.0,
                       help='variance_smooth æˆªæ–­ä¸Šç™¾åˆ†ä½ (é»˜è®¤95)')
    parser.add_argument('--weights-smooth-window', type=int, default=5,
                       help='variance_smooth å¹³æ»‘çª—å£(å¥‡æ•°ï¼Œé»˜è®¤5)')
    # AEå‚æ•°
    parser.add_argument('--ae-encoder-layers', type=int, nargs='+', default=None,
                       help='AEç¼–ç å™¨å±‚ï¼Œå¦‚ 64 32 16 8')
    parser.add_argument('--ae-decoder-layers', type=int, nargs='+', default=None,
                       help='AEè§£ç å™¨å±‚ï¼Œå¦‚ 16 32 64 81')
    parser.add_argument('--ae-alpha', type=float, default=None, help='AE L2æ­£åˆ™alpha')
    parser.add_argument('--ae-early-stopping', action='store_true', help='å¯ç”¨AE early_stopping')
    parser.add_argument('--ae-n-iter-no-change', type=int, default=None, help='early_stoppingå®¹å¿è¿­ä»£æ•°')
    parser.add_argument('--ae-validation-fraction', type=float, default=None, help='early_stoppingéªŒè¯é›†æ¯”ä¾‹')
    # æƒé‡ç›¸å…³
    parser.add_argument('--weights_base_range', type=float, nargs=2, metavar=('LO','HI'),
                       default=None, help='åŸºç¡€æƒé‡åŒºé—´ï¼Œä¾‹å¦‚ 400 680')
    parser.add_argument('--weights_base_value', type=float, default=None,
                       help='åŸºç¡€æƒé‡å€¼ï¼Œä¾‹å¦‚ 3.0')
    parser.add_argument('--weights_peak_range', type=float, nargs=2, metavar=('LO','HI'),
                       default=None, help='é‡ç‚¹å¢å¼ºåŒºé—´ï¼Œä¾‹å¦‚ 400 550')
    parser.add_argument('--weights_peak_multiplier', type=float, default=None,
                       help='é‡ç‚¹åŒºé—´å€ç‡ï¼Œä¾‹å¦‚ 1.5')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = SpectrumAnomalyTrainer(args.coating_name, args.version)
    
    # æ›´æ–°é…ç½®
    trainer.config.update({
        'training_samples': args.training_samples,
        'quality_threshold': args.quality_threshold,
        'stability_threshold_percentile': args.stability_threshold_percentile,
        'stability_threshold_method': args.stability_threshold_method,
        'stability_mad_k': args.stability_mad_k,
    })
    trainer.config['weights_mode'] = (args.weights_mode or 'static')
    trainer.config['weights_mix_alpha'] = args.weights_mix_alpha
    trainer.config['weights_smooth_pctl_lo'] = args.weights_smooth_pctl_lo
    trainer.config['weights_smooth_pctl_hi'] = args.weights_smooth_pctl_hi
    trainer.config['weights_smooth_window'] = args.weights_smooth_window
    # è¦†ç›–æƒé‡é…ç½®ï¼ˆä»…å½“æä¾›æ—¶ï¼‰
    if args.weights_base_range is not None:
        trainer.config['weights_base_range'] = args.weights_base_range
    if args.weights_base_value is not None:
        trainer.config['weights_base_value'] = args.weights_base_value
    if args.weights_peak_range is not None:
        trainer.config['weights_peak_range'] = args.weights_peak_range
    if args.weights_peak_multiplier is not None:
        trainer.config['weights_peak_multiplier'] = args.weights_peak_multiplier
    
    # ä¼ é€’æ ‡å‡†æ›²çº¿æ±‡èšæ–¹æ³•
    setattr(trainer, 'std_curve_agg', args.std_curve_agg)
    # è‡ªå®šä¹‰æ•°æ®è·¯å¾„
    if args.data_npz:
        setattr(trainer, 'data_npz_path', args.data_npz)
    # è¦†ç›–AEé…ç½®
    if args.ae_encoder_layers is not None:
        trainer.config['ae_encoder_layers'] = args.ae_encoder_layers
    if args.ae_decoder_layers is not None:
        trainer.config['ae_decoder_layers'] = args.ae_decoder_layers
    if args.ae_alpha is not None:
        trainer.config['ae_alpha'] = args.ae_alpha
    if args.ae_early_stopping:
        trainer.config['ae_early_stopping'] = True
    if args.ae_n_iter_no_change is not None:
        trainer.config['ae_n_iter_no_change'] = args.ae_n_iter_no_change
    if args.ae_validation_fraction is not None:
        trainer.config['ae_validation_fraction'] = args.ae_validation_fraction
    logger.info(f"è®­ç»ƒé…ç½®: {trainer.config} | æ ‡å‡†æ›²çº¿èšåˆ: {args.std_curve_agg}")
    
    # æ‰§è¡Œè®­ç»ƒ
    result = trainer.train()
    
    # è¾“å‡ºç»“æœ
    if result['status'] == 'SUCCESS':
        print("\n" + "="*60)
        print("ğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆï¼")
        print("="*60)
        print(f"æ¶‚å±‚: {args.coating_name}")
        print(f"ç‰ˆæœ¬: {args.version}")
        print(f"è®­ç»ƒæ—¶é—´: {result['training_time']:.2f} ç§’")
        print(f"Quality Scoreé˜ˆå€¼: {result['report']['similarity_evaluator']['quality_threshold']}%")
        print(f"Stability Scoreé˜ˆå€¼: {result['report']['weighted_autoencoder']['stability_threshold']:.6f}")
        print("\nä¿å­˜çš„æ–‡ä»¶:")
        for file_type, file_path in result['saved_files'].items():
            print(f"  - {file_type}: {os.path.basename(file_path)}")
    else:
        print("\n" + "="*60)
        print("âŒ è®­ç»ƒå¤±è´¥ï¼")
        print("="*60)
        print(f"é”™è¯¯: {result['error']}")
    
    return result['status'] == 'SUCCESS'

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)