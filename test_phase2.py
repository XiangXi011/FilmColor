#!/usr/bin/env python3
"""
SimilarityEvaluator æµ‹è¯•è„šæœ¬
éªŒè¯Quality Scoreè®¡ç®—ã€æƒé‡è®¡ç®—ç­‰åŠŸèƒ½
"""

import sys
import os
import numpy as np
import pandas as pd

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/workspace/code/spectrum_anomaly_detection')

def test_similarity_evaluator():
    """æµ‹è¯•SimilarityEvaluatoråŠŸèƒ½"""
    print("=" * 60)
    print("Phase 2: SimilarityEvaluator æµ‹è¯•")
    print("=" * 60)
    
    try:
        # å¯¼å…¥è¯„ä¼°å™¨
        from algorithms.similarity_evaluator import SimilarityEvaluator
        
        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = SimilarityEvaluator("DVP")
        print("âœ“ SimilarityEvaluator åˆå§‹åŒ–æˆåŠŸ")
        
        # åŠ è½½å¤„ç†åçš„DVPæ•°æ®
        data_path = "/workspace/code/spectrum_anomaly_detection/data/dvp_processed_data.npz"
        data = np.load(data_path)
        wavelengths = data['wavelengths']
        dvp_standard = data['dvp_values']
        
        print(f"âœ“ åŠ è½½DVPæ•°æ®: {len(wavelengths)}ä¸ªæ³¢é•¿ç‚¹")
        print(f"âœ“ æ³¢é•¿èŒƒå›´: {wavelengths.min():.0f}-{wavelengths.max():.0f}nm")
        print(f"âœ“ DVPåå°„ç‡èŒƒå›´: {dvp_standard.min():.4f}-{dvp_standard.max():.4f}")
        
        # æµ‹è¯•1: ç›¸åŒå…‰è°±è¯„ä¼°ï¼ˆåŸºå‡†æµ‹è¯•ï¼‰
        print("\nğŸ“Š æµ‹è¯•1: ç›¸åŒå…‰è°±è¯„ä¼°ï¼ˆåŸºå‡†æµ‹è¯•ï¼‰")
        print("-" * 40)
        
        result_identical = evaluator.evaluate(dvp_standard, dvp_standard, wavelengths, coating_name="DVP")
        print(f"âœ“ Quality Score: {result_identical['similarity_score_percent']:.2f}%")
        print(f"âœ“ åŠ æƒçš®å°”é€Šç›¸å…³ç³»æ•°: {result_identical['weighted_pearson']:.4f}")
        print(f"âœ“ åŠ æƒRMSE: {result_identical['rmse']:.6f}")
        
        if result_identical['similarity_score_percent'] > 99.9:
            print("âœ“ åŸºå‡†æµ‹è¯•é€šè¿‡ï¼šç›¸åŒå…‰è°±å¾—åˆ°æ¥è¿‘100%çš„åˆ†æ•°")
        else:
            print("âš ï¸  åŸºå‡†æµ‹è¯•å¼‚å¸¸ï¼šç›¸åŒå…‰è°±åˆ†æ•°ä½äºé¢„æœŸ")
        
        # æµ‹è¯•2: è½»å¾®å™ªå£°å…‰è°±è¯„ä¼°
        print("\nğŸ” æµ‹è¯•2: è½»å¾®å™ªå£°å…‰è°±è¯„ä¼°")
        print("-" * 40)
        
        noise_levels = [0.05, 0.1, 0.2, 0.5]
        noise_results = []
        
        for noise_level in noise_levels:
            noisy_spectrum = dvp_standard + np.random.normal(0, noise_level, len(dvp_standard))
            result = evaluator.evaluate(noisy_spectrum, dvp_standard, wavelengths, coating_name="DVP")
            noise_results.append({
                'noise_level': noise_level,
                'quality_score': result['similarity_score_percent'],
                'pearson': result['weighted_pearson'],
                'rmse': result['rmse']
            })
            print(f"  å™ªå£°æ°´å¹³ {noise_level:.2f}: Quality Score = {result['similarity_score_percent']:.2f}%")
        
        # æµ‹è¯•3: æƒé‡åˆ†æ
        print("\nâš–ï¸  æµ‹è¯•3: æƒé‡åˆ†æ")
        print("-" * 40)
        
        weight_data = evaluator.get_weight_visualization_data(wavelengths, "DVP")
        print(f"âœ“ æƒé‡èŒƒå›´: {weight_data['weight_stats']['min']:.2f} - {weight_data['weight_stats']['max']:.2f}")
        print(f"âœ“ æƒé‡å‡å€¼: {weight_data['weight_stats']['mean']:.2f}")
        print(f"âœ“ æƒé‡æ ‡å‡†å·®: {weight_data['weight_stats']['std']:.2f}")
        
        # ç»Ÿè®¡ä¸åŒæƒé‡å€¼çš„åˆ†å¸ƒ
        weight_counts = {}
        for w in weight_data['weights']:
            if w == 1.0:
                weight_counts['åŸºç¡€æƒé‡(1.0)'] = weight_counts.get('åŸºç¡€æƒé‡(1.0)', 0) + 1
            elif abs(w - 3.0) < 0.1:
                weight_counts['èŒƒå›´æƒé‡(3.0)'] = weight_counts.get('èŒƒå›´æƒé‡(3.0)', 0) + 1
            elif abs(w - 4.5) < 0.1:
                weight_counts['DVPå¢å¼ºæƒé‡(4.5)'] = weight_counts.get('DVPå¢å¼ºæƒé‡(4.5)', 0) + 1
            else:
                weight_counts['å…¶ä»–æƒé‡'] = weight_counts.get('å…¶ä»–æƒé‡', 0) + 1
        
        print("âœ“ æƒé‡åˆ†å¸ƒ:")
        for weight_type, count in weight_counts.items():
            percentage = count / len(weight_data['weights']) * 100
            print(f"  - {weight_type}: {count}ä¸ªç‚¹ ({percentage:.1f}%)")
        
        # æµ‹è¯•4: è´¨é‡é˜ˆå€¼
        print("\nğŸ¯ æµ‹è¯•4: è´¨é‡é˜ˆå€¼")
        print("-" * 40)
        
        quality_levels = ['excellent', 'good', 'acceptable', 'poor']
        thresholds = {}
        
        for level in quality_levels:
            threshold = evaluator.get_quality_score_threshold("DVP", level)
            thresholds[level] = threshold
            print(f"âœ“ {level.capitalize()}: {threshold}%")
        
        # æµ‹è¯•5: æ‰¹é‡è¯„ä¼°
        print("\nğŸ“ˆ æµ‹è¯•5: æ‰¹é‡è¯„ä¼°")
        print("-" * 40)
        
        # ç”Ÿæˆæµ‹è¯•å…‰è°±é›†åˆ
        test_spectra = []
        for i in range(10):
            if i < 3:
                # å‰3ä¸ªæ˜¯é«˜è´¨é‡å…‰è°±ï¼ˆä½å™ªå£°ï¼‰
                noise = np.random.normal(0, 0.05, len(dvp_standard))
            elif i < 7:
                # ä¸­é—´4ä¸ªæ˜¯ä¸­ç­‰è´¨é‡å…‰è°±ï¼ˆä¸­ç­‰å™ªå£°ï¼‰
                noise = np.random.normal(0, 0.2, len(dvp_standard))
            else:
                # æœ€å3ä¸ªæ˜¯ä½è´¨é‡å…‰è°±ï¼ˆé«˜å™ªå£°ï¼‰
                noise = np.random.normal(0, 0.5, len(dvp_standard))
            
            test_spectra.append(dvp_standard + noise)
        
        # æ‰§è¡Œæ‰¹é‡è¯„ä¼°
        batch_results = evaluator.batch_evaluate(test_spectra, dvp_standard, wavelengths, "DVP")
        
        print(f"âœ“ æ‰¹é‡è¯„ä¼°å®Œæˆ: {len(test_spectra)}ä¸ªå…‰è°±")
        print(f"âœ“ å¹³å‡Quality Score: {batch_results['similarity_score_percent'].mean():.2f}%")
        print(f"âœ“ Scoreæ ‡å‡†å·®: {batch_results['similarity_score_percent'].std():.2f}%")
        print(f"âœ“ æœ€é«˜åˆ†: {batch_results['similarity_score_percent'].max():.2f}%")
        print(f"âœ“ æœ€ä½åˆ†: {batch_results['similarity_score_percent'].min():.2f}%")
        
        # æŒ‰è´¨é‡ç­‰çº§åˆ†ç±»
        excellent_count = (batch_results['similarity_score_percent'] >= thresholds['excellent']).sum()
        good_count = (batch_results['similarity_score_percent'] >= thresholds['good']).sum()
        acceptable_count = (batch_results['similarity_score_percent'] >= thresholds['acceptable']).sum()
        
        print(f"âœ“ è´¨é‡ç­‰çº§åˆ†å¸ƒ:")
        print(f"  - Excellent (â‰¥{thresholds['excellent']}%): {excellent_count}ä¸ª")
        print(f"  - Good (â‰¥{thresholds['good']}%): {good_count}ä¸ª")
        print(f"  - Acceptable (â‰¥{thresholds['acceptable']}%): {acceptable_count}ä¸ª")
        print(f"  - Poor (<{thresholds['acceptable']}%): {len(test_spectra) - acceptable_count}ä¸ª")
        
        # ä¿å­˜æµ‹è¯•ç»“æœ
        print("\nğŸ’¾ ä¿å­˜æµ‹è¯•ç»“æœ")
        print("-" * 40)
        
        # ä¿å­˜æƒé‡å¯è§†åŒ–æ•°æ®
        weight_viz_path = "/workspace/code/spectrum_anomaly_detection/output/dvp_weights_visualization.npz"
        os.makedirs(os.path.dirname(weight_viz_path), exist_ok=True)
        np.savez_compressed(weight_viz_path, **weight_data)
        print(f"âœ“ æƒé‡å¯è§†åŒ–æ•°æ®å·²ä¿å­˜: {weight_viz_path}")
        
        # ä¿å­˜æ‰¹é‡è¯„ä¼°ç»“æœ
        batch_results_path = "/workspace/code/spectrum_anomaly_detection/output/dvp_batch_evaluation_results.csv"
        batch_results.to_csv(batch_results_path, index=False)
        print(f"âœ“ æ‰¹é‡è¯„ä¼°ç»“æœå·²ä¿å­˜: {batch_results_path}")
        
        # ç”Ÿæˆæµ‹è¯•æ€»ç»“æŠ¥å‘Š
        summary = {
            'phase': 'Phase 2: SimilarityEvaluator æµ‹è¯•',
            'status': 'COMPLETED',
            'test_results': {
                'baseline_test': {
                    'identical_spectrum_score': result_identical['similarity_score_percent'],
                    'passed': result_identical['similarity_score_percent'] > 99.9
                },
                'noise_test': {
                    'noise_levels_tested': noise_levels,
                    'results': noise_results
                },
                'weight_analysis': {
                    'weight_stats': weight_data['weight_stats'],
                    'weight_distribution': weight_counts
                },
                'quality_thresholds': thresholds,
                'batch_evaluation': {
                    'total_spectra': len(test_spectra),
                    'average_score': float(batch_results['similarity_score_percent'].mean()),
                    'score_std': float(batch_results['similarity_score_percent'].std()),
                    'quality_distribution': {
                        'excellent': int(excellent_count),
                        'good': int(good_count),
                        'acceptable': int(acceptable_count),
                        'poor': int(len(test_spectra) - acceptable_count)
                    }
                }
            },
            'next_steps': [
                'Phase 3: åŠ æƒè‡ªç¼–ç å™¨æ¨¡å‹å¼€å‘',
                'Phase 4: æ¨¡å‹è®­ç»ƒè„šæœ¬å¼€å‘'
            ]
        }
        
        summary_path = "/workspace/code/spectrum_anomaly_detection/output/phase2_test_summary.json"
        import json
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        print(f"âœ“ æµ‹è¯•æ€»ç»“æŠ¥å‘Šå·²ä¿å­˜: {summary_path}")
        
        print("\nğŸ‰ Phase 2 æµ‹è¯•å®Œæˆï¼")
        print("âœ“ SimilarityEvaluator åŠŸèƒ½éªŒè¯é€šè¿‡")
        print("âœ“ Quality Score è®¡ç®—æ­£ç¡®")
        print("âœ“ æƒé‡è®¡ç®—ç¬¦åˆDVPæ¶‚å±‚ç‰¹å¾")
        print("âœ“ æ‰¹é‡è¯„ä¼°åŠŸèƒ½æ­£å¸¸")
        print("âœ“ å¯ä»¥å¼€å§‹ Phase 3")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ Phase 2 æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    test_similarity_evaluator()