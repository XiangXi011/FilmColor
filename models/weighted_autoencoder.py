"""
Weighted Autoencoder - åŠ æƒè‡ªç¼–ç å™¨æ¨¡å‹
ä¸“é—¨ç”¨äºDVPæ¶‚å±‚ç±»å‹çš„å…‰è°±å¼‚å¸¸æ£€æµ‹

å®ç°è§„æ ¼ï¼š
- æ¶æ„: 81â†’48â†’16â†’4â†’16â†’48â†’81 (å…¨è¿æ¥ç½‘ç»œ)
- æŸå¤±å‡½æ•°: åŠ æƒå‡æ–¹è¯¯å·® (Weighted MSE)
- é¢„å¤„ç†: StandardScaler
- è®­ç»ƒ: EarlyStoppingå›è°ƒ
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, callbacks
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import joblib
import json
import os
from typing import Dict, Any, Tuple, Optional, List
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# è®¾ç½®TensorFlowæ—¥å¿—çº§åˆ«
tf.get_logger().setLevel('WARNING')

class WeightedAutoencoder:
    """
    åŠ æƒè‡ªç¼–ç å™¨æ¨¡å‹
    
    ç”¨äºDVPæ¶‚å±‚ç±»å‹çš„å…‰è°±å¼‚å¸¸æ£€æµ‹ï¼Œé€šè¿‡é‡æ„è¯¯å·®è¯„ä¼°è¿‡ç¨‹ç¨³å®šæ€§
    """
    
    def __init__(self, input_dim: int = 81, coating_name: str = "DVP"):
        """
        åˆå§‹åŒ–åŠ æƒè‡ªç¼–ç å™¨
        
        Args:
            input_dim: è¾“å…¥ç»´åº¦ï¼ˆæ³¢é•¿ç‚¹æ•°ï¼‰
            coating_name: æ¶‚å±‚åç§°ï¼Œç”¨äºæƒé‡è®¡ç®—
        """
        self.input_dim = input_dim
        self.coating_name = coating_name
        self.model = None
        self.scaler = StandardScaler()
        self.is_trained = False
        
        # æ¨¡å‹é…ç½®
        self.encoder_dims = [48, 16, 4]  # ç¼–ç å™¨ç»´åº¦
        self.decoder_dims = [16, 48]     # è§£ç å™¨ç»´åº¦
        self.latent_dim = 4              # æ½œåœ¨ç©ºé—´ç»´åº¦
        
        # è®­ç»ƒé…ç½®
        self.learning_rate = 1e-3
        self.epochs = 100
        self.batch_size = 32
        self.validation_split = 0.2
        self.early_stopping_patience = 10
        
        logger.info(f"WeightedAutoencoder åˆå§‹åŒ–å®Œæˆ [{coating_name}]")
        logger.info(f"è¾“å…¥ç»´åº¦: {input_dim}, æ½œåœ¨ç»´åº¦: {self.latent_dim}")
    
    def build_model(self) -> keras.Model:
        """
        æ„å»ºåŠ æƒè‡ªç¼–ç å™¨æ¨¡å‹
        
        Returns:
            keras.Model: æ„å»ºçš„æ¨¡å‹
        """
        # è¾“å…¥å±‚
        input_layer = layers.Input(shape=(self.input_dim,), name='spectrum_input')
        
        # ç¼–ç å™¨
        encoded = input_layer
        for i, dim in enumerate(self.encoder_dims):
            encoded = layers.Dense(
                dim, 
                activation='relu', 
                name=f'encoder_layer_{i+1}'
            )(encoded)
        
        # æ½œåœ¨ç©ºé—´
        latent = layers.Dense(
            self.latent_dim, 
            activation='relu', 
            name='latent_space'
        )(encoded)
        
        # è§£ç å™¨
        decoded = latent
        for i, dim in enumerate(self.decoder_dims):
            decoded = layers.Dense(
                dim, 
                activation='relu', 
                name=f'decoder_layer_{i+1}'
            )(decoded)
        
        # è¾“å‡ºå±‚ï¼ˆçº¿æ€§æ¿€æ´»ï¼‰
        output_layer = layers.Dense(
            self.input_dim, 
            activation=None, 
            name='reconstructed_spectrum'
        )(decoded)
        
        # åˆ›å»ºæ¨¡å‹
        model = keras.Model(input_layer, output_layer, name=f'weighted_autoencoder_{self.coating_name}')
        
        # ç¼–è¯‘æ¨¡å‹
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=self.learning_rate),
            loss=self.weighted_mse_loss,
            metrics=['mse']
        )
        
        logger.info(f"æ¨¡å‹æ„å»ºå®Œæˆ: {model.count_params():,} å‚æ•°")
        return model
    
    def weighted_mse_loss(self, y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:
        """
        è‡ªå®šä¹‰åŠ æƒå‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°
        
        Args:
            y_true: çœŸå®å…‰è°±æ•°æ®
            y_pred: é‡æ„å…‰è°±æ•°æ®
            
        Returns:
            tf.Tensor: åŠ æƒæŸå¤±å€¼
        """
        # è·å–æƒé‡ï¼ˆéœ€è¦åœ¨è®­ç»ƒæ—¶æä¾›ï¼‰
        # è¿™é‡Œä½¿ç”¨å…¨å±€æƒé‡ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦åœ¨fitæ–¹æ³•ä¸­ä¼ é€’
        weights = getattr(self, 'current_weights', tf.ones_like(y_true))
        
        # è®¡ç®—åŠ æƒå‡æ–¹è¯¯å·®
        squared_diff = tf.square(y_true - y_pred)
        weighted_squared_diff = weights * squared_diff
        
        return tf.reduce_mean(weighted_squared_diff)
    
    def prepare_weights(self, wavelengths: np.ndarray, coating_name: str = None) -> np.ndarray:
        """
        å‡†å¤‡æƒé‡å‘é‡ï¼ˆç”¨äºæŸå¤±å‡½æ•°ï¼‰
        
        Args:
            wavelengths: æ³¢é•¿æ•°ç»„
            coating_name: æ¶‚å±‚åç§°
            
        Returns:
            np.ndarray: æƒé‡å‘é‡
        """
        if coating_name is None:
            coating_name = self.coating_name
        
        # ä½¿ç”¨ä¸SimilarityEvaluatorç›¸åŒçš„æƒé‡è®¡ç®—é€»è¾‘
        weights = np.ones_like(wavelengths, dtype=np.float64)
        
        # åŸºç¡€æƒé‡: 400-680nmèŒƒå›´æƒé‡ä¸º3
        mask = (wavelengths >= 400) & (wavelengths <= 680)
        weights[mask] = 3.0
        
        # æ ¹æ®æ¶‚å±‚ç±»å‹è°ƒæ•´æƒé‡
        if coating_name == "DVP":
            # DVPæ¶‚å±‚: å¢å¼º400-550nmæ³¢æ®µçš„æƒé‡
            peak_mask = (wavelengths >= 400) & (wavelengths <= 550)
            weights[peak_mask] *= 1.5
        
        # å½’ä¸€åŒ–æƒé‡
        weights = weights / np.mean(weights)
        
        logger.debug(f"æƒé‡å‡†å¤‡å®Œæˆ: min={weights.min():.2f}, max={weights.max():.2f}, mean={weights.mean():.2f}")
        return weights
    
    def preprocess_data(self, spectra: np.ndarray, wavelengths: np.ndarray, 
                       fit_scaler: bool = True) -> np.ndarray:
        """
        æ•°æ®é¢„å¤„ç†
        
        Args:
            spectra: å…‰è°±æ•°æ® (n_samples, n_features)
            wavelengths: æ³¢é•¿æ•°ç»„
            fit_scaler: æ˜¯å¦æ‹ŸåˆStandardScaler
            
        Returns:
            np.ndarray: é¢„å¤„ç†åçš„æ•°æ®
        """
        if fit_scaler:
            logger.info("æ‹ŸåˆStandardScaler...")
            spectra_scaled = self.scaler.fit_transform(spectra)
        else:
            logger.info("ä½¿ç”¨å·²æ‹Ÿåˆçš„StandardScaler...")
            spectra_scaled = self.scaler.transform(spectra)
        
        logger.info(f"æ•°æ®é¢„å¤„ç†å®Œæˆ: {spectra.shape} -> {spectra_scaled.shape}")
        return spectra_scaled
    
    def train(self, normal_spectra: np.ndarray, wavelengths: np.ndarray,
             validation_split: Optional[float] = None, verbose: int = 1) -> Dict[str, Any]:
        """
        è®­ç»ƒåŠ æƒè‡ªç¼–ç å™¨æ¨¡å‹
        
        Args:
            normal_spectra: æ­£å¸¸å…‰è°±æ•°æ® (n_samples, n_features)
            wavelengths: æ³¢é•¿æ•°ç»„
            validation_split: éªŒè¯é›†æ¯”ä¾‹
            verbose: è®­ç»ƒè¯¦ç»†ç¨‹åº¦
            
        Returns:
            Dict[str, Any]: è®­ç»ƒå†å²å’Œç»“æœ
        """
        if validation_split is None:
            validation_split = self.validation_split
        
        logger.info(f"å¼€å§‹è®­ç»ƒæ¨¡å‹: {normal_spectra.shape[0]}ä¸ªæ­£å¸¸æ ·æœ¬")
        
        # å‡†å¤‡æƒé‡
        weights = self.prepare_weights(wavelengths, self.coating_name)
        
        # æ•°æ®é¢„å¤„ç†
        spectra_scaled = self.preprocess_data(normal_spectra, wavelengths, fit_scaler=True)
        
        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        if validation_split > 0:
            X_train, X_val = train_test_split(
                spectra_scaled, 
                test_size=validation_split, 
                random_state=42
            )
            logger.info(f"æ•°æ®åˆ’åˆ†: è®­ç»ƒé›†{len(X_train)}ä¸ª, éªŒè¯é›†{len(X_val)}ä¸ª")
        else:
            X_train = spectra_scaled
            X_val = None
            logger.info("ä½¿ç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒï¼Œæ— éªŒè¯é›†")
        
        # æ„å»ºæ¨¡å‹
        if self.model is None:
            self.model = self.build_model()
        
        # è®¾ç½®å½“å‰æƒé‡ï¼ˆç”¨äºæŸå¤±å‡½æ•°ï¼‰
        self.current_weights = tf.constant(weights, dtype=tf.float32)
        
        # å‡†å¤‡å›è°ƒå‡½æ•°
        callback_list = [
            callbacks.EarlyStopping(
                monitor='val_loss',
                patience=self.early_stopping_patience,
                restore_best_weights=True,
                verbose=1
            ),
            callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=5,
                min_lr=1e-6,
                verbose=1
            )
        ]
        
        # è®­ç»ƒæ¨¡å‹
        logger.info("å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
        history = self.model.fit(
            X_train, X_train,
            validation_data=(X_val, X_val) if X_val is not None else None,
            epochs=self.epochs,
            batch_size=self.batch_size,
            callbacks=callback_list,
            verbose=verbose
        )
        
        # æ ‡è®°ä¸ºå·²è®­ç»ƒ
        self.is_trained = True
        
        # è®¡ç®—è®­ç»ƒé›†å’ŒéªŒè¯é›†çš„é‡æ„è¯¯å·®
        train_predictions = self.model.predict(X_train, verbose=0)
        train_errors = self.calculate_reconstruction_errors(X_train, train_predictions, weights)
        
        if X_val is not None:
            val_predictions = self.model.predict(X_val, verbose=0)
            val_errors = self.calculate_reconstruction_errors(X_val, val_predictions, weights)
        else:
            val_errors = None
        
        # è®¡ç®—é˜ˆå€¼ï¼ˆ99.5%åˆ†ä½æ•°ï¼‰
        if val_errors is not None:
            threshold = np.percentile(val_errors, 99.5)
            logger.info(f"é˜ˆå€¼è®¡ç®—å®Œæˆ: 99.5%åˆ†ä½æ•° = {threshold:.6f}")
        else:
            threshold = np.percentile(train_errors, 99.5)
            logger.info(f"é˜ˆå€¼è®¡ç®—å®Œæˆ: 99.5%åˆ†ä½æ•° = {threshold:.6f} (åŸºäºè®­ç»ƒé›†)")
        
        training_result = {
            'training_history': history.history,
            'final_train_loss': float(history.history['loss'][-1]),
            'final_val_loss': float(history.history['val_loss'][-1]) if 'val_loss' in history.history else None,
            'train_reconstruction_errors': train_errors.tolist(),
            'val_reconstruction_errors': val_errors.tolist() if val_errors is not None else None,
            'threshold': float(threshold),
            'epochs_trained': len(history.history['loss']),
            'weights_used': weights.tolist(),
            'model_config': {
                'input_dim': self.input_dim,
                'encoder_dims': self.encoder_dims,
                'decoder_dims': self.decoder_dims,
                'latent_dim': self.latent_dim,
                'learning_rate': self.learning_rate,
                'batch_size': self.batch_size,
                'coating_name': self.coating_name
            }
        }
        
        logger.info(f"è®­ç»ƒå®Œæˆ: {training_result['epochs_trained']}ä¸ªepoch, æœ€ç»ˆæŸå¤±={training_result['final_train_loss']:.6f}")
        return training_result
    
    def calculate_reconstruction_errors(self, X: np.ndarray, X_pred: np.ndarray, 
                                      weights: np.ndarray) -> np.ndarray:
        """
        è®¡ç®—é‡æ„è¯¯å·®ï¼ˆStability Scoreï¼‰
        
        Args:
            X: åŸå§‹å…‰è°±æ•°æ®
            X_pred: é‡æ„å…‰è°±æ•°æ®
            weights: æƒé‡å‘é‡
            
        Returns:
            np.ndarray: é‡æ„è¯¯å·®æ•°ç»„
        """
        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„é‡æ„è¯¯å·®
        squared_diff = (X - X_pred) ** 2
        weighted_squared_diff = weights * squared_diff
        reconstruction_errors = np.mean(weighted_squared_diff, axis=1)
        
        return reconstruction_errors
    
    def predict(self, spectra: np.ndarray, wavelengths: np.ndarray) -> Dict[str, Any]:
        """
        é¢„æµ‹å’Œå¼‚å¸¸æ£€æµ‹
        
        Args:
            spectra: å¾…é¢„æµ‹çš„å…‰è°±æ•°æ® (n_samples, n_features)
            wavelengths: æ³¢é•¿æ•°ç»„
            
        Returns:
            Dict[str, Any]: é¢„æµ‹ç»“æœ
        """
        if not self.is_trained:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨trainæ–¹æ³•")
        
        # æ•°æ®é¢„å¤„ç†
        spectra_scaled = self.preprocess_data(spectra, wavelengths, fit_scaler=False)
        
        # é‡æ„
        predictions = self.model.predict(spectra_scaled, verbose=0)
        
        # è®¡ç®—é‡æ„è¯¯å·®
        weights = self.prepare_weights(wavelengths, self.coating_name)
        reconstruction_errors = self.calculate_reconstruction_errors(spectra_scaled, predictions, weights)
        
        # è®¡ç®—æ¯ä¸ªæ³¢é•¿çš„é‡æ„è¯¯å·®ï¼ˆç”¨äºè¯Šæ–­ï¼‰
        per_wavelength_errors = []
        for i in range(len(spectra)):
            squared_diff = (spectra_scaled[i] - predictions[i]) ** 2
            weighted_squared_diff = weights * squared_diff
            per_wavelength_errors.append(weighted_squared_diff.tolist())
        
        results = {
            'reconstructed_spectra': predictions.tolist(),
            'reconstruction_errors': reconstruction_errors.tolist(),
            'per_wavelength_errors': per_wavelength_errors,
            'threshold': getattr(self, 'threshold', None),
            'metadata': {
                'coating_name': self.coating_name,
                'num_samples': len(spectra),
                'wavelength_range': f"{wavelengths.min():.0f}-{wavelengths.max():.0f}nm"
            }
        }
        
        logger.debug(f"é¢„æµ‹å®Œæˆ: {len(spectra)}ä¸ªæ ·æœ¬")
        return results
    
    def save_model(self, model_dir: str, version: str = "v1.0") -> Dict[str, str]:
        """
        ä¿å­˜æ¨¡å‹å’Œç›¸å…³ç»„ä»¶
        
        Args:
            model_dir: æ¨¡å‹ä¿å­˜ç›®å½•
            version: ç‰ˆæœ¬å·
            
        Returns:
            Dict[str, str]: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        if not self.is_trained:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒï¼Œæ— æ³•ä¿å­˜")
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        coating_dir = os.path.join(model_dir, self.coating_name, version)
        os.makedirs(coating_dir, exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹
        model_path = os.path.join(coating_dir, f"model_{self.coating_name}_{version}.h5")
        self.model.save(model_path)
        
        # ä¿å­˜é¢„å¤„ç†å™¨
        scaler_path = os.path.join(coating_dir, f"scaler_{self.coating_name}_{version}.pkl")
        joblib.dump(self.scaler, scaler_path)
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'coating_name': self.coating_name,
            'version': version,
            'input_dim': self.input_dim,
            'encoder_dims': self.encoder_dims,
            'decoder_dims': self.decoder_dims,
            'latent_dim': self.latent_dim,
            'learning_rate': self.learning_rate,
            'batch_size': self.batch_size,
            'training_date': pd.Timestamp.now().isoformat(),
            'threshold': getattr(self, 'threshold', None)
        }
        
        metadata_path = os.path.join(coating_dir, f"metadata_{self.coating_name}_{version}.json")
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        saved_files = {
            'model': model_path,
            'scaler': scaler_path,
            'metadata': metadata_path
        }
        
        logger.info(f"æ¨¡å‹ä¿å­˜å®Œæˆ: {coating_dir}")
        return saved_files
    
    def load_model(self, model_dir: str, version: str = "v1.0") -> None:
        """
        åŠ è½½æ¨¡å‹å’Œç›¸å…³ç»„ä»¶
        
        Args:
            model_dir: æ¨¡å‹ä¿å­˜ç›®å½•
            version: ç‰ˆæœ¬å·
        """
        # åŠ è½½æ¨¡å‹
        model_path = os.path.join(model_dir, self.coating_name, version, f"model_{self.coating_name}_{version}.h5")
        self.model = keras.models.load_model(
            model_path, 
            custom_objects={'weighted_mse_loss': self.weighted_mse_loss}
        )
        
        # åŠ è½½é¢„å¤„ç†å™¨
        scaler_path = os.path.join(model_dir, self.coating_name, version, f"scaler_{self.coating_name}_{version}.pkl")
        self.scaler = joblib.load(scaler_path)
        
        # åŠ è½½å…ƒæ•°æ®
        metadata_path = os.path.join(model_dir, self.coating_name, version, f"metadata_{self.coating_name}_{version}.json")
        with open(metadata_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        # è®¾ç½®é˜ˆå€¼
        self.threshold = metadata.get('threshold')
        
        self.is_trained = True
        
        logger.info(f"æ¨¡å‹åŠ è½½å®Œæˆ: {model_path}")
    
    def get_model_summary(self) -> str:
        """
        è·å–æ¨¡å‹æ‘˜è¦
        
        Returns:
            str: æ¨¡å‹æ‘˜è¦å­—ç¬¦ä¸²
        """
        if self.model is None:
            return "æ¨¡å‹å°šæœªæ„å»º"
        
        summary_lines = []
        self.model.summary(print_fn=lambda x: summary_lines.append(x))
        return '\n'.join(summary_lines)

# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    # åˆ›å»ºåŠ æƒè‡ªç¼–ç å™¨
    autoencoder = WeightedAutoencoder(input_dim=81, coating_name="DVP")
    
    # åŠ è½½DVPæ•°æ®
    data = np.load("/workspace/code/spectrum_anomaly_detection/data/dvp_processed_data.npz")
    wavelengths = data['wavelengths']
    dvp_standard = data['dvp_values']
    
    print("=" * 60)
    print("Weighted Autoencoder æµ‹è¯•")
    print("=" * 60)
    
    # ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼ˆæ­£å¸¸æ ·æœ¬ï¼‰
    print("\nç”Ÿæˆè®­ç»ƒæ•°æ®...")
    np.random.seed(42)
    n_samples = 100
    noise_levels = [0.05, 0.1, 0.15]  # ä½å™ªå£°æ°´å¹³è¡¨ç¤ºæ­£å¸¸æ ·æœ¬
    
    normal_spectra = []
    for i in range(n_samples):
        noise_level = np.random.choice(noise_levels)
        noise = np.random.normal(0, noise_level, len(dvp_standard))
        spectrum = dvp_standard + noise
        normal_spectra.append(spectrum)
    
    normal_spectra = np.array(normal_spectra)
    print(f"âœ“ ç”Ÿæˆ {len(normal_spectra)} ä¸ªæ­£å¸¸æ ·æœ¬")
    
    # è®­ç»ƒæ¨¡å‹
    print("\nè®­ç»ƒæ¨¡å‹...")
    training_result = autoencoder.train(normal_spectra, wavelengths, validation_split=0.2)
    
    print(f"âœ“ è®­ç»ƒå®Œæˆ: {training_result['epochs_trained']} ä¸ªepoch")
    print(f"âœ“ æœ€ç»ˆè®­ç»ƒæŸå¤±: {training_result['final_train_loss']:.6f}")
    if training_result['final_val_loss']:
        print(f"âœ“ æœ€ç»ˆéªŒè¯æŸå¤±: {training_result['final_val_loss']:.6f}")
    print(f"âœ“ é˜ˆå€¼: {training_result['threshold']:.6f}")
    
    # æµ‹è¯•é¢„æµ‹
    print("\næµ‹è¯•é¢„æµ‹...")
    test_spectrum = dvp_standard + np.random.normal(0, 0.1, len(dvp_standard))
    test_spectra = test_spectrum.reshape(1, -1)
    
    prediction_result = autoencoder.predict(test_spectra, wavelengths)
    reconstruction_error = prediction_result['reconstruction_errors'][0]
    
    print(f"âœ“ é‡æ„è¯¯å·®: {reconstruction_error:.6f}")
    print(f"âœ“ é˜ˆå€¼: {prediction_result['threshold']:.6f}")
    print(f"âœ“ å¼‚å¸¸æ£€æµ‹: {'æ˜¯' if reconstruction_error > prediction_result['threshold'] else 'å¦'}")
    
    # ä¿å­˜æ¨¡å‹
    print("\nä¿å­˜æ¨¡å‹...")
    saved_files = autoencoder.save_model("/workspace/code/spectrum_anomaly_detection/models", "v1.0")
    print(f"âœ“ æ¨¡å‹ä¿å­˜å®Œæˆ:")
    for key, path in saved_files.items():
        print(f"  - {key}: {os.path.basename(path)}")
    
    print("\nğŸ‰ Weighted Autoencoder æµ‹è¯•å®Œæˆï¼")